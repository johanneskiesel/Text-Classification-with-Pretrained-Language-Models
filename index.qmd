---
title:  Text Classification with Pretrained Language Models Tutorial
author: Stephan Linzbach (GESIS)
bibliography: references.bib
csl: apa.csl
format:
  html: default
  ipynb: default
---

# Text Classification Tutorial

## Learning Objectives

This tutorial has the following learning objectives:
-	Learning how to work with large language models (RoBERTa)
-	Customizing (fine-tuning) a large language model for a text classification task in any language (100+ languages supported)
-	Low-resource learning (with only few hundred examples) using the SAM optimizer


## Description

This step-by-step tutorial provides an accessible introduction to customizing (fine-tuning) a pre-trained multilingual language model (RoBERTa) for text classification tasks. It demonstrates how to use the model's existing knowledge to classify text accurately, even with a small set of labeled examples. It takes input as JSON files with text documents and their corresponding labels for training, validating and testing. It covers using specialized models for English, German, and French while employing XLM-RoBERTa for over 100 additional languages.

## Target Audience (Difficulty Level)

-	Social scientists willing to learn about using large language models with basic prior understanding of it
-	Social scientists with expertise in large-language models, interested in fine-tuning for multiple languages from only few examples.
-	Computer scientists interested in learning about how large-language models are used for social text classification.
-	Advanced NLP researchers and professors looking for tutorials that can help their students in learning new topics.

## Prerequisites

Use this tutorial preferably in [Google Colab](https://colab.research.google.com/github/Stephan-Linzbach/Text-Classification-with-Pretrained-Language-Models/blob/main/textclassification_tutorial.ipynb), as the setup depends on the pre-installed packages of the Colab environment.

## Environment Setup

To follow this tutorial, ensure the following:

- Python >= 3.7 installed.
- Install required libraries by running:
  ```bash
  pip install -r requirements.txt
  ```

## Tutorial Content

### 1. Introduction to the Task
- Explanation of single-label classification and its applications.

### 2. Data Preparation
- Loading the data
- Loading language-specific language model

```{python preparation}
language = input("Which language do you use? ('english', 'german', 'french') ")

print(f"MMhhh interesting your data is written in {language}. Let's load a fitting PLM!")

if language == 'english':
  model_name = "roberta-base"
elif language == 'german':
  model_name = "benjamin/roberta-base-wechsel-german"
elif language == 'french':
  model_name = "camembert-base"
else:
  print(f"Seems like we have no model available for {language}.")
  print("We will load a mulitlingual language model. It knows text from 100 languages.")
  model_name = 'xlm-roberta-base'

print(f"We loaded {model_name} for {language}.")

# Output:
# Which language do you use? ('english', 'german', 'french') english
# MMhhh interesting your data is written in english. Let's load a fitting PLM!
# We loaded roberta-base for english.
```



### 3. Defining the Classification Task
- Defining the way data points are classified.
- Setting the number of different labels.

```{python defining}
# Prompt the user to select the classification type
decision_type = input("Do you want to classify by 'choosing' or 'deciding'? ").strip().lower()

# Validate the input
assert decision_type in ['choosing', 'deciding'], "Invalid input! Please enter 'choosing' or 'deciding'."

# Ensure the labels align with the selected classification type
if decision_type == 'deciding':
    assert isinstance(train['Labels'][0], list), (
        "For 'deciding', labels should be a list (e.g., ['apple', 'banana'])."
    )
else:
    assert not isinstance(train['Labels'][0], list), (
        "For 'choosing', each label should be a single value (e.g., 'apple')."
    )
# Output:
# Do you want to classify by 'choosing' or 'deciding'? choosing
```

### 4. Setting up the Model
- Defining training parameters: batch size, learning rate, number of epochs.
- Defining training infrastructure: optimizers, a learning rate scheduler, a model, and a tokenizer.

```{python setup}
# Model configuration
model_config = {'pretrained_model_name_or_path': model_name,
                'num_labels': output_size,
                'problem_type': objective,
                'id2label': id2label,
                'label2id': label2id}

# Training Hyperparameters
batch_size = 64
lr = 1e-4
num_epochs = 3
warm_up_rate = 0.1
num_training_steps = (len(train['Labels'])//batch_size)*num_epochs
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
```

### 5. Training
- Implementing the training loop with learning rate scheduling and optimizer setup.
- Running the training and validation loop.
- Load the best-performing model.

```{python training}
for epoch in range(num_epochs): # Repeat num_epochs times

  for batch_idx, batch in enumerate(train_dataloader): # Train the model on the batch
    input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)

    output = model(input_ids, attention_mask, labels=y)
    loss = output.loss
    loss.backward()

# Output:
# Train: Epoch 2, Train step 4, Loss 0.09070442616939545, learning_rate 1.4303513272105057e-05
#  Train: Epoch 2, Train step 5, Loss 0.0538533590734005, learning_rate 3.7138015365554833e-06
# Train: Epoch 2, Train step 6, Loss 0.05279867351055145, learning_rate 0.0
# Validation: Epoch 2, Train step 6, Loss 0.050605375319719315, old best/epoch .0882/1
# **** END EPOCH 2 ****
# **** FINISHED TRAINING FOR N=2 ****
# BEST EPOCH: 2
# BEST LOSS: 0.050605375319719315
```



### 6. Evaluation
- Testing the model on unseen data.
- Generating a classification report with metrics (precision, recall, F1-score).
- Interpreting results and identifying potential improvements.
- Understanding the classification report (precision, recall, F1-score, and accuracy).
- Analyzing the model's performance on the test set.



```{python evaluation}
print(classification_report(test_y, y_pred, target_names=label2id.keys(), zero_division=True))

# Output:
#                precision    recall  f1-score   support
#
#   is_correct       1.00      0.00      0.00        64
# is_incorrect       0.67      1.00      0.80       128
#
#     accuracy                           0.67       192
#    macro avg       0.83      0.50      0.40       192
# weighted avg       0.78      0.67      0.53       192
```

## Duration

This tutorial will take approximately 1–2 hours.

## Social Science Use Cases 

A social scientist studying social media behavior over time. A training set should be created that teaches the classification of the desired information: political-leaning, factualness of argument, emotionality, stance towards a topic etc.

## Flow Diagram 

The flow involves:

1. Data Preparation.
2. Model configuration and initialization.
3. Training and validation.
4. Evaluation and result interpretation.

## How to Use

1. Clone the repository or download the notebook.
2. Follow the steps sequentially in the notebook to preprocess data, fine-tune the model, and evaluate it.
3. Modify hyperparameters or use a different dataset for custom tasks.

## Conclusion

This tutorial covers the end-to-end pipeline for text classification using fine-tuning of a pre-trained model. By following the steps, you will acquire the skills to apply similar techniques to your own datasets and tasks.

## References

Conneau, A. (2019). Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.
Liu, Y. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364.
Martin, L., Muller, B., Suárez, P. J. O., Dupont, Y., Romary, L., de La Clergerie, É. V. & Sagot, B. (2019). CamemBERT: a tasty French language model. arXiv preprint arXiv:1911.03894.
Minixhofer, B., Paischer, F., & Rekabsaz, N. (2021). WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models. arXiv preprint arXiv:2112.06598.
Foret, P., Kleiner, A., Mobahi, H., & Neyshabur, B. (2020). Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412.

## Contact Details

For questions or feedback, contact:

- **Stephan Linzbach**: [Stephan.Linzbach@gesis.org](mailto:Stephan.Linzbach@gesis.org)

## Acknowledgments

This tutorial uses resources from the Hugging Face library and PyTorch framework.

