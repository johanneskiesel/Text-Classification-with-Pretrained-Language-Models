{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanneskiesel/Text-Classification-with-Pretrained-Language-Models/blob/main/textclassification_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8222d6ZMz8Nm"
      },
      "source": [
        "# A practical guide to multilingual large language  model (RoBERTa) classification\n",
        "\n",
        "This step-by-step tutorial provides an accessible introduction to customizing (fine-tuning) a pre-trained multilingual language model (RoBERTa) for text classification tasks. It demonstrates how to use the model's existing knowledge to classify text accurately, even with a small set of labeled examples. It takes input as JSON files with text documents and their corresponding labels for training, validating and testing. It covers using specialized models for English, German, and French while employing XLM-RoBERTa for over 100 additional languages.\n",
        "\n",
        "# Learning Objectives\n",
        "\n",
        "This tutorial has the following learning objectives:\n",
        "-\tLearning how to work with large language models (RoBERTa)\n",
        "-\tCustomizing (fine-tuning) a large language model for a text classification task in any language (100+ languages supported)\n",
        "-\tLow-resource learning (with only few hundred examples) using the SAM optimizer\n",
        "\n",
        "\n",
        "# Target Audience\n",
        "-\tSocial scientists willing to learn about using large language models with basic prior understanding of it\n",
        "-\tSocial scientists with expertise in large-language models, interested in fine-tuning for multiple languages from only few examples.\n",
        "-\tComputer scientists interested in learning about how large-language models are used for social text classification.\n",
        "-\tAdvanced NLP researchers and professors looking for tutorials that can help their students in learning new topics.\n",
        "\n",
        "\n",
        "# Prerequisites\n",
        "Use this tutorial preferably in google colab, as the setup depends on the pre-installed packages of the colab environment.\n",
        "\n",
        "#Environment Setup\n",
        "Run the cells below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KuYVJkLn_b0",
        "outputId": "fdd5945f-6b22-42d3-a61b-b7f95d0ebcc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-10-10 20:07:01--  https://raw.githubusercontent.com/davda54/sam/main/sam.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2484 (2.4K) [text/plain]\n",
            "Saving to: ‘sam.py.2’\n",
            "\n",
            "sam.py.2            100%[===================>]   2.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-10 20:07:01 (41.5 MB/s) - ‘sam.py.2’ saved [2484/2484]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers\n",
        "!wget https://raw.githubusercontent.com/davda54/sam/main/sam.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZpvTl_jHGEi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader\n",
        "from sam import SAM\n",
        "import shutil\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGVQ7lJTloP9"
      },
      "outputs": [],
      "source": [
        "## Utils\n",
        "\n",
        "\n",
        "def convert_in_output_size(labels, mapping):\n",
        "    label_resized = []\n",
        "    for l in labels:\n",
        "        tmp_l = torch.tensor([1 if k in l else 0 for k in mapping])\n",
        "        label_resized.append(tmp_l)\n",
        "    label_resized = torch.stack(label_resized, dim=0)\n",
        "    return label_resized\n",
        "def convert_labels(labels, mapping):\n",
        "    if isinstance(labels[0], list):\n",
        "        labels = convert_in_output_size(labels, mapping)\n",
        "    return torch.tensor([mapping[l] for l in labels])\n",
        "\n",
        "def flatten_list(list_to_flatten):\n",
        "  \"\"\"\n",
        "  Returns one list from a list of lists.\n",
        "  \"\"\"\n",
        "  return [x for xs in list_to_flatten for x in xs]\n",
        "\n",
        "def infer_output_size(data):\n",
        "  \"\"\"\n",
        "  Returns the number of possible labels and the possible labels.\n",
        "  \"\"\"\n",
        "  labels = data['Labels']\n",
        "  if isinstance(labels[0], list):\n",
        "    labels = flatten_list(labels)\n",
        "  labels = set(labels)\n",
        "  return len(labels), labels\n",
        "\n",
        "def generate_dataloader(text, y, batch_size, workers=1):\n",
        "  \"\"\"\n",
        "  Returns a dataloader with input_ids and attention_mask to process the text.\n",
        "  \"\"\"\n",
        "  attention_mask = text['attention_mask']\n",
        "  input_ids = text['input_ids']\n",
        "\n",
        "  dataset = list(zip(input_ids, attention_mask, y))\n",
        "\n",
        "  dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=workers)\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abq2zMSNIs6y"
      },
      "source": [
        "# Tutorial Content\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUCEaj7BDygM"
      },
      "source": [
        "## 1) Introduction\n",
        "\n",
        "We will start with the most pressing questions:\n",
        "  - What exactly is a **text-classification**?\n",
        "  - What is a **pre-trained language model**?\n",
        "  - What even is **fine-tuning**?\n",
        "\n",
        "We will answer all questions in the following text.\n",
        "\n",
        "In **text-classification** we try to assign a property to a text.\n",
        "\n",
        "For example we are interested in classifying texts that are about fruits.\n",
        "We could easily find a dictionary with all fruits (e.g.: 'Apple', 'Banana', 'Pear' etc.) everytime we recognize such a word in a text we know this text is about fruits, right?\n",
        "However, this might not be true all the time for example \"Apple designed the new pencil pro.\" is not about the fruit 'Apple' although we would recognize it as such with our dictionary approach.\n",
        "Furthermore, this would only work for the language in the dictionary. However, our tutorial is helpful for 100+ languages.\n",
        "So the context of the word might be helpful (more on this later).\n",
        "Classification is obviously transferable to more than just fruits.\n",
        "People try to classify the sentiment of a text, the stance towards an entity expressed in a text, the topic of a text, the expressed emotion in a text, and many many more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCBUwjg5DwTW"
      },
      "source": [
        "## 2) Data Preparation\n",
        "\n",
        "Let's talk data:\n",
        "In order to make this script work you have to save three dictionaries in this structure in the file '<current_folder>/(train|val|test).json':\n",
        "\n",
        "  ```python\n",
        "  {'Text' : [list of texts]\n",
        "   'Labels': [list of labels]}\n",
        "  ```\n",
        "\n",
        " Each text document in the data should have a corresponding label such that:\n",
        "\n",
        "  ```python\n",
        "  length([list of texts]) == length([list of labels])\n",
        "  ```\n",
        "\n",
        "Example:\n",
        " ```python\n",
        " {'Text': ['Yesterday i ate an apple.', 'Yesterday I crashed my Apple.'],\n",
        "  'Labels': ['about_fruit', 'not_about_fruit']}\n",
        "  ```\n",
        "\n",
        "The *train data* is used to teach the model, the *val data* is required to validate if the model understands the train data correctly and the *test data* is used to proof the capabilities of the final version of the model on the **unseen** *test set*.\n",
        "\n",
        "\n",
        "You can classify text into one category (single label classification) or several categories per text (multilabel classification).\n",
        "\n",
        "\n",
        "Is your data ready? Then lets start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRaIwoOMIrtX",
        "outputId": "a29b6e33-86bd-4218-9650-5641bcaddf51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We loaded the train data with 192 texts and 192 labels,\n",
            "the validation data with 192 texts and 192 labels\n",
            "and the test data with 192 texts and 192 labels.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  with open(\"./train.json\") as f:\n",
        "    train = json.load(f)\n",
        "\n",
        "  with open(\"./val.json\") as f:\n",
        "    val = json.load(f)\n",
        "\n",
        "  with open(\"./test.json\") as f:\n",
        "    test = json.load(f)\n",
        "except:\n",
        "  #Dummy Values\n",
        "  train = {'Text': [\"An Apple is a Fruit!\", \"An Apple is not a Fruit!\", \"An Apple has no seeds.\"]*64,\n",
        "          'Labels': ['is_correct', 'is_incorrect', 'is_incorrect']*64}\n",
        "  val = {'Text': [\"An Apple is not a Fruit!\", \"An Apple is a Fruit!\", \"An Apple has no seeds.\"]*64,\n",
        "          'Labels': ['is_incorrect', 'is_correct', 'is_incorrect']*64}\n",
        "  test = {'Text': [\"An Apple is not a Fruit!\", \"An Apple is has no seeds.\", \"An Apple is a Fruit.\"]*64,\n",
        "          'Labels': ['is_incorrect', 'is_incorrect', 'is_correct']*64}\n",
        "\n",
        "\n",
        "assert len(train['Text']) == len(train['Labels']), \"Number of texts does not match number of labels for train data!\"\n",
        "assert len(val['Text']) == len(val['Labels']), \"Number of texts does not match number of labels for val data!\"\n",
        "assert len(test['Text']) == len(test['Labels']), \"Number of texts does not match number of labels for test data!\"\n",
        "\n",
        "print(f\"We loaded the train data with {len(train['Text'])} texts and {len(train['Labels'])} labels,\")\n",
        "print(f\"the validation data with {len(val['Text'])} texts and {len(val['Labels'])} labels\")\n",
        "print(f\"and the test data with {len(test['Text'])} texts and {len(test['Labels'])} labels.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WQ19YMNEAuK"
      },
      "source": [
        "Great the data is ready!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woCqHJzKELOP"
      },
      "source": [
        "### Understanding the Data\n",
        "You have to answer some questions about your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BYdtu44Xrhp"
      },
      "source": [
        "\n",
        "\n",
        "#### Finding a Language Specific Language Model\n",
        "\n",
        "In which language is your text data written?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHzEmeSnXq2w",
        "outputId": "e99877e8-bf0e-4f06-cfce-876ac2c2643e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MMhhh interesting your data is written in english. Let's load a fitting PLM!\n",
            "We loaded roberta-base for english.\n"
          ]
        }
      ],
      "source": [
        "language = 'english' # language to use e.g., 'english', 'german', 'french'\n",
        "\n",
        "print(f\"MMhhh interesting your data is written in {language}. Let's load a fitting PLM!\")\n",
        "\n",
        "if language == 'english':\n",
        "  model_name = \"roberta-base\"\n",
        "elif language == 'german':\n",
        "  model_name = \"benjamin/roberta-base-wechsel-german\"\n",
        "elif language == 'french':\n",
        "  model_name = \"camembert-base\"\n",
        "else:\n",
        "  print(f\"Seems like we have no model available for {language}.\")\n",
        "  print(\"We will load a multilingual language model. It knows text from 100 languages.\")\n",
        "  model_name = 'xlm-roberta-base'\n",
        "\n",
        "print(f\"We loaded {model_name} for {language}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq68uB4cbnKc"
      },
      "source": [
        "Ok now that we talked about the language of your data you might be intersted what the ``model_name`` stands for.\n",
        "\n",
        "These are **pre-trained language models** ready to be used with your specific language.\n",
        "These language models already learned to understand language by solving a huge cloze-text written in the particular language.\n",
        "\n",
        "This cloze-text is constructed over Wikipedia or other huge text datasets.\n",
        "\n",
        "Now that we understood what **text classification** and **pre-trained language model** are.\n",
        "\n",
        "We can now talk about the last question: What is **fine-tuning**?\n",
        "\n",
        "As you might imagine solving a cloze-text over the whole internet makes you knowledgeable but not an expert in a field. We now want to transform our **pre-trained language model** into an expert for your task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhm0nAieFGRl"
      },
      "source": [
        "## 3) Defining the Classification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pm-wgbhEarc"
      },
      "source": [
        "\n",
        "### Choosing vs. Deciding\n",
        "\n",
        "Before starting, it’s essential to clarify what type of classification task you want to perform. We distinguish between two main tasks:\n",
        "\n",
        "#### 1. Choosing (Single-Label Classification)\n",
        "- **Example:** *What is your favorite fruit?*\n",
        "- You select one correct label from a list of possible options.\n",
        "\n",
        "#### 2. Deciding (Multi-Label Classification)\n",
        "- **Example:** *Do you like apples?*\n",
        "- You evaluate each label independently and decide whether it applies.\n",
        "\n",
        "\n",
        "In our dataset, we have **single labels**, so we will insert `'choosing'` as the classification type. Here's the code for defining your task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP2b9HkBY1Qc"
      },
      "outputs": [],
      "source": [
        "# Prompt the user to select the classification type\n",
        "decision_type =  'choosing' # classify by 'choosing' or 'deciding'?\n",
        "\n",
        "# Validate the input\n",
        "assert decision_type in ['choosing', 'deciding'], \"Invalid input! Please enter 'choosing' or 'deciding'.\"\n",
        "\n",
        "# Ensure the labels align with the selected classification type\n",
        "if decision_type == 'deciding':\n",
        "    assert isinstance(train['Labels'][0], list), (\n",
        "        \"For 'deciding', labels should be a list (e.g., ['apple', 'banana']).\"\n",
        "    )\n",
        "else:\n",
        "    assert not isinstance(train['Labels'][0], list), (\n",
        "        \"For 'choosing', each label should be a single value (e.g., 'apple').\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFBY_wgsg6wg"
      },
      "source": [
        "### Choosing the Loss and Decision Functions\n",
        "\n",
        "Based on your ``decision_type`` we will now choose the correct loss function and the decision function.\n",
        "The loss function tells the model how well it achieved your task.\n",
        "The decision function tells us how to convert the model guesses in the actual decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZD7XiARg6KS"
      },
      "outputs": [],
      "source": [
        "losses = {'deciding': \"multi_label_classification\", #torch.nn.CrossEntropyLoss(),\n",
        "          'choosing': \"single_label_classification\",}\n",
        "\n",
        "decisions = {'deciding': lambda x: torch.where(x > 0, 1, 0),\n",
        "             'choosing': lambda x: torch.argmax(x, dim=1)}\n",
        "\n",
        "objective = losses[decision_type]\n",
        "decision_function = decisions[decision_type]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcFgWRZiM8o"
      },
      "source": [
        "### Fixing the Number of Possible Answers:\n",
        "\n",
        "The next question we need to clarify is: How many different labels are possible for your task?\n",
        "\n",
        "Example:\n",
        "\n",
        "Choose your favorite fruit from this list:\n",
        "  ```python\n",
        "  poss_labels = ['Banana', 'Apple', 'Pear', 'Peach'`]\n",
        "  model_output = [0.4, 0.5, -0.1, 0.7]\n",
        "  ```\n",
        "\n",
        "Or decide if you like the particular fruit:\n",
        "  ```python\n",
        "  poss_labels = ['Banana', 'Apple', 'Pear', 'Peach'`]\n",
        "  model_output = [0.4, 0.5, -0.1, 0.7]\n",
        "  ```\n",
        "\n",
        "In both cases we have **4** possible labels.\n",
        "In the first case we choose the fruit where the model signals the biggest aggreement **'Peach'** in the second we decide for each fruit if we like it by chosing to like everything above 0. Therefore, our example output tells us that we like **'Banana'**, **'Apple'**, and **'Peach'**.\n",
        "\n",
        "\n",
        "\n",
        "Let's find out how many labels are possible in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6AKMt2JiLd7",
        "outputId": "38d9f462-6259-45ad-b841-1f1d0a499d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your task distinguishes 2 different labels. These are: {'is_correct', 'is_incorrect'}\n"
          ]
        }
      ],
      "source": [
        "# Ask user to define the number of possible labels\n",
        "output_size = \"2\" # Number of labels\n",
        "\n",
        "# Ensure the user input is a valid integer\n",
        "def new_func(output_size):\n",
        "    assert output_size.isdigit(), \"Output_size needs to be a natural number.\"\n",
        "\n",
        "new_func(output_size)\n",
        "output_size = int(output_size)\n",
        "\n",
        "# Infer the number of labels and possible labels from the datasets\n",
        "infered_output_size_train, possible_labels_train = infer_output_size(train)\n",
        "infered_output_size_val, possible_labels_val = infer_output_size(val)\n",
        "infered_output_size_test, possible_labels_test = infer_output_size(test)\n",
        "\n",
        "# Determine the maximum number of inferred labels\n",
        "possible_labels = max(infered_output_size_train, infered_output_size_val, infered_output_size_test)\n",
        "\n",
        "# Warn the user if datasets have inconsistent labels\n",
        "if not (possible_labels_train == possible_labels_val == possible_labels_test):\n",
        "    print(\n",
        "        f\"Warning: Train contains {possible_labels_train} possible labels, \"\n",
        "        f\"Val contains {possible_labels_val} possible labels, and \"\n",
        "        f\"Test contains {possible_labels_test} possible labels. \"\n",
        "        \"This inconsistency is not recommended. All datasets should ideally contain the same labels.\"\n",
        "    )\n",
        "\n",
        "# Ensure the output size matches the inferred number of labels\n",
        "assert output_size == possible_labels, (\n",
        "    f\"We inferred {possible_labels} with the following possible labels: {possible_labels_train}.\"\n",
        ")\n",
        "assert possible_labels_train == possible_labels_val == possible_labels_test, (\n",
        "    \"Make sure that train, val, and test labels are equal!\"\n",
        ")\n",
        "\n",
        "# Create mapping dictionaries for labels and IDs\n",
        "id2label = {i: k for i, k in enumerate(possible_labels_train)}\n",
        "label2id = {k: i for i, k in enumerate(possible_labels_train)}\n",
        "\n",
        "print(f\"Your task distinguishes {output_size} different labels. These are: {possible_labels_train}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYAu-jSLtSBf"
      },
      "source": [
        "## 4) Setting up the Model\n",
        "\n",
        "\n",
        "Fantastic!\n",
        "\n",
        "We are close.\n",
        "We clarified the language, the objective and number of possible answers.\n",
        "\n",
        "Now, let’s load the necessary model and tokenizer.  \n",
        "The **tokenizer** translates the language into a model-specific vocabulary that the model can process efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGtsc2tGtQWi"
      },
      "outputs": [],
      "source": [
        "model_config = {'pretrained_model_name_or_path': model_name,\n",
        "                'num_labels': output_size,\n",
        "                'problem_type': objective,\n",
        "                'id2label': id2label,\n",
        "                'label2id': label2id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "1ebedda1ae3e4496b70ce5580ac09149",
            "73a38f3157a84e6787570a24f0b1556f",
            "0fc91d76b14d4a8d89b9dd3a21f5450c",
            "c4f6ec88c3014b6bbba1fd8515ff1507",
            "6912ccbab616408da9887a6d9967a4ae",
            "44ce5c02767c4e98b1d0af7fdb3928c2"
          ]
        },
        "id": "IDYrCWQwHBS4",
        "outputId": "4477a192-3a39-42f4-da32-49e5be5434c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ebedda1ae3e4496b70ce5580ac09149",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73a38f3157a84e6787570a24f0b1556f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fc91d76b14d4a8d89b9dd3a21f5450c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4f6ec88c3014b6bbba1fd8515ff1507",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6912ccbab616408da9887a6d9967a4ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44ce5c02767c4e98b1d0af7fdb3928c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(**model_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLFf-h69n8Nn"
      },
      "source": [
        "### Training Specific Settings\n",
        "\n",
        "Now we need to specify some training-specific parameters.  \n",
        "Don’t worry if you’re unsure about what to change—the preset values should work just fine for most cases.\n",
        "\n",
        "#### A Quick Intuition on Training\n",
        "\n",
        "When training a model, you provide it with some example data points (in your case, the training data). From this data, the model learns helpful patterns that explain the correlation between input and output.  \n",
        "To make the most of the data:  \n",
        "- We feed small portions (batches) to the model at a time, controlled by the **batch_size**.  \n",
        "- The model uses these batches to infer patterns but commits to those patterns cautiously, guided by the **learning rate (lr)**.  \n",
        "- To ensure the model retains knowledge from pre-training, we use a **warm-up rate**, which helps the model transition smoothly without forgetting its pre-trained knowledge.  \n",
        "\n",
        "#### Key Parameters\n",
        "Let’s go through each parameter one by one:  \n",
        "- **batch_size**: Determines how many examples we show to the model before deducing rules to improve classification.  \n",
        "- **learning_rate (lr)**: Controls how strongly the model commits to patterns it recognizes within each batch.  \n",
        "- **num_epoch**: Specifies how many times the model sees all the training data (e.g., 3 times).  \n",
        "- **warm_up_rate**: Indicates the portion of training during which the model makes smaller adjustments.  \n",
        "- **Device**: Refers to the hardware used for training. If you have a GPU, your training will be much faster.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWLgh6LRGNOE"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "num_epochs = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "warm_up_rate = 0.1\n",
        "num_training_steps = (len(train['Labels'])//batch_size)*num_epochs\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w37QzRN_U41u"
      },
      "source": [
        "\n",
        "\n",
        "For now we only need the train-set to teach the model and the val-set to decide if we taught our model well.\n",
        "Lets first translate our text into the models vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP_Jj34oz9r"
      },
      "outputs": [],
      "source": [
        "# Padding tokenizing text aka. translating text into the model vocabulary\n",
        "\n",
        "train_text = tokenizer([m for m in train['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "val_text = tokenizer([m for m in val['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "test_text = tokenizer([m for m in test['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "\n",
        "# Using the label2id mapping to convert the label strings into label ids\n",
        "train_y = convert_labels(train['Labels'], label2id)\n",
        "val_y = convert_labels(val['Labels'], label2id)\n",
        "test_y = convert_labels(test['Labels'], label2id)\n",
        "\n",
        "# Retrieve Dataloaders for fast iteration over the data\n",
        "train_dataloader = generate_dataloader(train_text, train_y, batch_size)\n",
        "val_dataloader = generate_dataloader(val_text, val_y, batch_size)\n",
        "test_dataloader = generate_dataloader(test_text, test_y, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELylkOMArO-h"
      },
      "source": [
        "The learning of patterns and adaptation of the model is achieved by the optimizer. In our case it is a special optimizer that keeps a model from optimizing. If you are really interested you can read more about it [here](https://github.com/davda54/sam). The scheduler adapts the learning rate according the warm_up_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFuqFXajGfJ6"
      },
      "outputs": [],
      "source": [
        "# Initialize optimizer and scheduler\n",
        "optimizer = SAM(model.parameters(), torch.optim.Adam, lr=lr, adaptive=True)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer = optimizer,\n",
        "                                          num_warmup_steps = num_training_steps*warm_up_rate,\n",
        "                                          num_training_steps = num_training_steps,\n",
        "                                          last_epoch = -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_hiMENYrwM0"
      },
      "source": [
        "## 5) Fine-tuning (Training)\n",
        "\n",
        "Let’s start the training process!  \n",
        "\n",
        "During fine-tuning, we show the training data to the model and adjust its parameters to optimize performance for the task.  \n",
        "Here’s what happens:  \n",
        "- The model learns patterns in the data to perform the classification task.  \n",
        "- After each **epoch** (a complete pass through the training dataset), we test the model on the validation set to monitor progress.  \n",
        "- The best-performing model is saved during the training process.\n",
        "\n",
        "Once training is complete, you will have a well-trained model ready for use.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUwFFftDV457",
        "outputId": "f1dc2b7e-7818-4690-cb2e-8a583d51fb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Memory: 184.25 GB total, 159.00 GB available\n",
            "No GPU available - using CPU\n",
            "Model has 124,647,170 parameters\n"
          ]
        }
      ],
      "source": [
        "# Memory and system check before training\n",
        "import psutil\n",
        "import gc\n",
        "# Trainings-loop with memory management fixes\n",
        "import os\n",
        "\n",
        "# Set tokenizers parallelism to avoid fork warnings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Check system memory\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"System Memory: {memory.total / 1024**3:.2f} GB total, {memory.available / 1024**3:.2f} GB available\")\n",
        "\n",
        "# Check GPU memory if available\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    gpu_memory_allocated = torch.cuda.memory_allocated(0)\n",
        "    gpu_memory_reserved = torch.cuda.memory_reserved(0)\n",
        "\n",
        "    print(f\"GPU Memory: {gpu_memory / 1024**3:.2f} GB total\")\n",
        "    print(f\"GPU Memory Allocated: {gpu_memory_allocated / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory Reserved: {gpu_memory_reserved / 1024**3:.2f} GB\")\n",
        "\n",
        "    # Clear any existing GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "else:\n",
        "    print(\"No GPU available - using CPU\")\n",
        "\n",
        "# Check model size\n",
        "model_size = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model has {model_size:,} parameters\")\n",
        "\n",
        "# Recommend batch size based on available memory\n",
        "if torch.cuda.is_available():\n",
        "    available_gpu_memory = gpu_memory - gpu_memory_reserved\n",
        "    if available_gpu_memory < 4 * 1024**3:  # Less than 4GB available\n",
        "        recommended_batch_size = 8\n",
        "    elif available_gpu_memory < 8 * 1024**3:  # Less than 8GB available\n",
        "        recommended_batch_size = 16\n",
        "    else:\n",
        "        recommended_batch_size = 32\n",
        "\n",
        "    print(f\"Recommended batch size: {recommended_batch_size}\")\n",
        "\n",
        "    if batch_size > recommended_batch_size:\n",
        "        print(f\"Warning: Current batch size ({batch_size}) may be too large. Consider reducing to {recommended_batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e35OhfF0JO52"
      },
      "outputs": [],
      "source": [
        "\n",
        "best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "already_trained = 0\n",
        "best_model_path = ''\n",
        "should_delete = True\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs): # Repeat num_epochs times\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader): # Train the model on the batch\n",
        "        try:\n",
        "            input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "\n",
        "            # First forward pass\n",
        "            output = model(input_ids, attention_mask, labels=y)\n",
        "            loss = output.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # SAM optimizer first step\n",
        "            optimizer.first_step(zero_grad=True)\n",
        "\n",
        "            # Second forward pass (required by SAM)\n",
        "            output2 = model(input_ids, attention_mask, labels=y)\n",
        "            loss2 = output2.loss\n",
        "            loss2.backward()\n",
        "\n",
        "            # SAM optimizer second step\n",
        "            optimizer.second_step(zero_grad=True)\n",
        "\n",
        "            # Update learning rate AFTER optimizer steps\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Train: Epoch {epoch}, Train step {already_trained+batch_idx}, Loss {loss.item():.4f}, learning_rate {scheduler.get_last_lr()[0]:.2e}\", flush=True)\n",
        "\n",
        "            # Clear cache periodically to prevent memory buildup\n",
        "            if batch_idx % 5 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"OOM Error at batch {batch_idx}. Trying to recover...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                # Try with smaller effective batch size\n",
        "                if batch_size > 8:\n",
        "                    batch_size = batch_size // 2\n",
        "                    print(f\"Reducing batch size to {batch_size}\")\n",
        "                    train_dataloader = generate_dataloader(train_text, train_y, batch_size)\n",
        "                    val_dataloader = generate_dataloader(val_text, val_y, batch_size)\n",
        "                    break\n",
        "                else:\n",
        "                    raise e\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    already_trained += batch_idx\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_dataloader): # Validate the current state of the model on the validation data\n",
        "            input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            val_output = model(input_ids, attention_mask, labels=y)\n",
        "            val_loss.append(val_output.loss)\n",
        "\n",
        "    val_loss = torch.mean(torch.stack(val_loss))\n",
        "\n",
        "    print(f\"Validation: Epoch {epoch}, Train step {already_trained}, Loss {val_loss.item():.4f}, old best/epoch {str(best_loss)[1:6]}/{best_epoch}\", flush=True)\n",
        "\n",
        "    if val_loss < best_loss: # Save the model if the val_loss is the best loss we have seen so far\n",
        "        best_loss = val_loss.item()\n",
        "        best_epoch = epoch\n",
        "        if should_delete and best_model_path and os.path.exists(best_model_path):\n",
        "            shutil.rmtree(best_model_path)\n",
        "        best_model_path = f\"./my_model_epoch_{best_epoch}_val_loss_{str(val_loss.item())[1:6]}\"\n",
        "        model.save_pretrained(best_model_path, from_pt=True)\n",
        "\n",
        "        print(f\"**** END EPOCH {epoch} ****\")\n",
        "\n",
        "    # Clean up memory after each epoch\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"**** FINISHED TRAINING FOR N={num_epochs} ****\")\n",
        "print(f\"BEST EPOCH: {best_epoch}\")\n",
        "print(f\"BEST LOSS: {best_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6IoeYsasTlt"
      },
      "source": [
        "The training is finished now we can load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlEDLy1qCONM"
      },
      "outputs": [],
      "source": [
        "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8bn_AVsXY7"
      },
      "source": [
        "## 6) Evaluation\n",
        "\n",
        "Finally, with the loaded model we can now predict our results for the unseen test set to understand the models performance in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF0nHp1aRaHc"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_idx, batch in enumerate(test_dataloader):\n",
        "    input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "    y_pred.append(model(input_ids, attention_mask, labels=y).logits)\n",
        "\n",
        "\n",
        "y_pred = torch.cat(y_pred, dim=0)\n",
        "y_pred = decision_function(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXp0j0r5Tg4y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_y, y_pred, target_names=label2id.keys(), zero_division=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBm7kAZoAOz"
      },
      "source": [
        "### Results\n",
        "\n",
        "The classification report shows us four metric results these are the precision,\n",
        "the recall, the f1-score, and the accuracy. Additionally, the report displays two different average aggregations, these are the macro avg, and the weighted average.\n",
        "\n",
        "The *precision* tells us \"When we predict a label, is it the correct label?\".\n",
        "\n",
        "The *recall* tells us \"How many instances of a class do we find?\".\n",
        "\n",
        "The *f1-score* is the harmonic mean of the *precision* and the *recall*.\n",
        "\n",
        "The *accuracy* tells us \"How many of our predictions are correct?\".\n",
        "\n",
        "The *macro avg* aggregates the *f1-score* per class it tells us \"How well do we classify, if all classes occur equally often.\".\n",
        "\n",
        "The *weighted avg* aggregates the *f1-score* weighted by class size it tells us \"How well do we classify the complete label set.\".\n",
        "\n",
        "\n",
        "### Analysis\n",
        "We can see that we have two classes 'is_correct' and 'is_incorrect'.\n",
        "We have one instance with from the class 'is_correct' and two from the class 'is_incorrect'.\n",
        "Our model does not learn to predict the class 'is_correct'.\n",
        "We find all instances of the class 'is_incorrect'.\n",
        "You can see this by the fact that the *recall* is 1.0.\n",
        "The *precision* is at 0.67, as one instance that we predict to be 'is_incorrect' is actually an instance of the class 'is_correct'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Reading\n",
        "\n",
        "- [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/pdf/1911.02116)\n",
        "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692)\n",
        "- [CamemBERT: a Tasty French Language Model](https://arxiv.org/pdf/1911.03894)\n",
        "- [WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models](https://aclanthology.org/2022.naacl-main.293.pdf)\n",
        "- [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/pdf/2010.01412)"
      ],
      "metadata": {
        "id": "IR1ldehFV7xV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-G0nQF57ePk"
      },
      "source": [
        "## Contact Details\n",
        " For questions or feedback, contact Stephan Linzbach via [Stephan.Linzbach@gesis.org](mailto:Stephan.Linzbach@gesis.org)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "textclassification_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}