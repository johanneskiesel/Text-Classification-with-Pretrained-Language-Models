{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8222d6ZMz8Nm"
      },
      "source": [
        "# A practical guide to multilingual large language  model (RoBERTa) classification\n",
        "\n",
        "This step-by-step tutorial provides a gentle introduction to customizing (fine-tuning) a pre-trained multilingual language model (RoBERTa) for text classification tasks. It demonstrates how to use the model's existing knowledge to classify text accurately, even with a small set of labeled examples. It takes input as text documents and their corresponding labels for training, validating and testing. It covers using specialized models for English, German, and French, but employs XLM-RoBERTa, which can be used for over 100 languages.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "This tutorial answers the following questions:\n",
        "\n",
        "- What exactly is text-classification?\n",
        "- What is a pre-trained language model?\n",
        "- What even is fine-tuning?\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "-\tWork with large language models for text classification (RoBERTa)\n",
        "-\tCustomize (fine-tunine) a large language model for a text classification task in any language (100+ languages supported)\n",
        "-\tEmploy low-resource learning (with only few hundred examples) using the SAM optimizer\n",
        "\n",
        "## Target Audience\n",
        "\n",
        "This tutorial is aimed at an intermediate level. You should have basic knowledge of large language models and of Python programming.\n",
        "\n",
        "## Duration\n",
        "\n",
        "About half a work-day.\n",
        "\n",
        "## Use Cases\n",
        "\n",
        "- Training and then using a text classifier for special text data, for example to detect sentiment of specific texts, hatefulness of social media posts or something completely different like whether a text contains mentions of fruits or not.\n",
        "\n",
        "## Environment Setup\n",
        "\n",
        "Use this tutorial preferably in an environment with GPU access like Colab.\n",
        "\n",
        "Run the cells below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6KuYVJkLn_b0"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers pandas\n",
        "\n",
        "# download the SAM optimizer code\n",
        "!wget --quiet https://raw.githubusercontent.com/davda54/sam/main/sam.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-ZpvTl_jHGEi"
      },
      "outputs": [],
      "source": [
        "# Import packages used in the tutorial\n",
        "\n",
        "import pandas\n",
        "from sam import SAM\n",
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUCEaj7BDygM"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "What exactly is **text-classification**?\n",
        "\n",
        "In text-classification we try to assign a property to a text.\n",
        "\n",
        "For example we are interested in classifying texts that are about fruits. We could easily find a dictionary with all fruits (e.g.: 'Apple', 'Banana', 'Pear' etc.). Everytime we recognize a word from this dictionary in a text we know this text is about fruits, right? However, this might not be true all the time. For example \"Apple designed the new pencil pro.\" is not about the fruit 'Apple' although we would recognize it as such with our dictionary approach. So the context of the word might be helpful (more on this later). Furthermore, dictionaries work only for the language in the dictionary. However, we aim for multilingual approaches in this tutorial and will thus replace dictionaries with AI methods later in this tutorial.\n",
        "\n",
        "Classification is obviously transferable to more than just fruits. People try to classify the sentiment of a text, the stance towards an entity expressed in a text, the topic of a text, the expressed emotion in a text, and many many more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCBUwjg5DwTW"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "We here focus on text-classification in its purest form, i.e., have as data points single texts, each of which is assigned a single label (or class) that our classifier has to predict (as opposed to multi-label classification, in which a text can be assigned none up to all available labels, see the \"Defining the Classification Task\" section). We use the following format for this tutorial:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'Text': [\"text1\", \"text2\", \"...\", \"textN\"]\n",
        "  'Labels': [\"label-of-text1\", \"label-of-text2\", \"...\", \"label-of-textN\"]\n",
        "}\n",
        "```\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'Text': ['Yesterday i ate an apple.', 'Yesterday I crashed my Apple.'],\n",
        "  'Labels': ['about_fruit', 'not_about_fruit']\n",
        "}\n",
        "```\n",
        "\n",
        "As in most classification settings, we separate our data into three parts:\n",
        "\n",
        "- train. Contains the data on which classifiers are based (\"training data\")\n",
        "- val. Contains the data based on which we select a classifier among the available ones (\"validation data\")\n",
        "- test. Contains the data to give us an estimate of the performance of the selected classifier (\"test data\")\n",
        "\n",
        "These different datasets should all be independent from each other, so that (1) we select the classifier that generalizes best to the validation data it does not know from the training data and (2) we get a solid performance estimate of the classifier in the wild from the test data (that was neither used to train nor to validate/select the classifier).\n",
        "\n",
        "We take an old Twitter [Bag Brands Sentiment Dataset](https://doi.org/10.5281/zenodo.7679325) as data. Feel free to use your own data instead."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://zenodo.org/records/7679325/files/bag_brand_sentiment_dataset.xlsx\n",
        "\n",
        "data = pandas.read_excel('bag_brand_sentiment_dataset.xlsx')\n",
        "data.head()  # have a look at the first 5 texts"
      ],
      "metadata": {
        "id": "514Du_SGpdj1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "286614f4-cc65-42fd-deaf-783179621729"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                                  0  \\\n",
              "0           1  WhEn I‚Äôm AbLe to PuRchAse A ChaNel BaG &amp; S...   \n",
              "1           2  (INFO) Taehyung is wearing CHANEL TOTE BAG ,GR...   \n",
              "2           3  Influencers on insta are like \"I finally got o...   \n",
              "3           4  @Brieyonce This is like Chanel‚Äôs advent calend...   \n",
              "4           5  Tae with jw anderson jacket and chanel tote ba...   \n",
              "\n",
              "              tweet_id                                               text  \\\n",
              "0           2740216059  when i‚Äôm able to purchase a chanel bag amp sti...   \n",
              "1  1591185253737053952  info taehyung is wearing chanel tote bag graff...   \n",
              "2             15441716  influencers on insta are like i finally got of...   \n",
              "3  1316674861658329088  brieyonce this is like chanel‚Äôs advent calenda...   \n",
              "4  1539547580299742976  tae with jw anderson jacket and chanel tote ba...   \n",
              "\n",
              "              user        location sentiment  \n",
              "0     blk_goddess8  Washington, DC  positive  \n",
              "1        Thv_style             NaN  positive  \n",
              "2         ac_palma        Portugal  negative  \n",
              "3       MarahhJayy             NaN   neutral  \n",
              "4  Bobabobabooboo1             NaN  positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c632aa3-3e3f-4e70-b2cd-6fad0680162e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>user</th>\n",
              "      <th>location</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>WhEn I‚Äôm AbLe to PuRchAse A ChaNel BaG &amp;amp; S...</td>\n",
              "      <td>2740216059</td>\n",
              "      <td>when i‚Äôm able to purchase a chanel bag amp sti...</td>\n",
              "      <td>blk_goddess8</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>(INFO) Taehyung is wearing CHANEL TOTE BAG ,GR...</td>\n",
              "      <td>1591185253737053952</td>\n",
              "      <td>info taehyung is wearing chanel tote bag graff...</td>\n",
              "      <td>Thv_style</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Influencers on insta are like \"I finally got o...</td>\n",
              "      <td>15441716</td>\n",
              "      <td>influencers on insta are like i finally got of...</td>\n",
              "      <td>ac_palma</td>\n",
              "      <td>Portugal</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>@Brieyonce This is like Chanel‚Äôs advent calend...</td>\n",
              "      <td>1316674861658329088</td>\n",
              "      <td>brieyonce this is like chanel‚Äôs advent calenda...</td>\n",
              "      <td>MarahhJayy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Tae with jw anderson jacket and chanel tote ba...</td>\n",
              "      <td>1539547580299742976</td>\n",
              "      <td>tae with jw anderson jacket and chanel tote ba...</td>\n",
              "      <td>Bobabobabooboo1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c632aa3-3e3f-4e70-b2cd-6fad0680162e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5c632aa3-3e3f-4e70-b2cd-6fad0680162e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5c632aa3-3e3f-4e70-b2cd-6fad0680162e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f0fed1a2-40cd-44f1-a7d1-42f9729a3869\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0fed1a2-40cd-44f1-a7d1-42f9729a3869')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f0fed1a2-40cd-44f1-a7d1-42f9729a3869 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 2881,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 831,\n        \"min\": 1,\n        \"max\": 2881,\n        \"num_unique_values\": 2881,\n        \"samples\": [\n          472,\n          1454,\n          2379\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2881,\n        \"samples\": [\n          \"Chanel Double Flap Bag Metallic Bronze Patent Leather - 100% Authentic \\n\\n Chanel Double Flap Bag Metallic Bro... \\nhttps://t.co/gZEyhJ1KHc \\n\\nhttps://t.co/gZEyhJ1KHc\",\n          \"What Gucci say?? \\n\\n\\u201cShe remind me of a very very expensive bag of weed\\u201d \\ud83e\\udee0\",\n          \"Authentic Louis Vuitton Monogram Petit Noe Shoulder Bag M40818 Used F/S https://t.co/nJyinPLrMX eBay https://t.co/T9Hq4T0oeA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 655442916267891840,\n        \"min\": 44493,\n        \"max\": 1601996724133839104,\n        \"num_unique_values\": 1995,\n        \"samples\": [\n          1130330889009787008,\n          922439549833400320,\n          1117929511042072960\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2710,\n        \"samples\": [\n          \"brooshie marekzee realnenadlukic unusualwhales all evidence comes from supreme leader elon and must not be refuted as his words are more feile than a bag of dogecoins\",\n          \"i\\u2019m so deep in my duffy in my bag in my prada\",\n          \"authentic chanel matelasse eah cc mark turn lock hand bag black lambskin authentic chanel matelasse eah cc \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1995,\n        \"samples\": [\n          \"bitchierek\",\n          \"EmHutz1\",\n          \"Scarletp16\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 972,\n        \"samples\": [\n          \"   RIP Bailey    \\u2665 9/13/14 \\u2665\",\n          \"Surrey, British Columbia\",\n          \"Hitchin, England\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first column contains the text, whereas the `sentiment` column contains the label.\n",
        "\n",
        "We now transform it into our simple structure detailed above:"
      ],
      "metadata": {
        "id": "1q2ymRmNuilv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_all = {\n",
        "    \"Text\": data[0].tolist(),\n",
        "    \"Labels\": data[\"sentiment\"].tolist()\n",
        "}\n",
        "num_data_all = len(data_all[\"Text\"])\n",
        "\n",
        "# It is always good to check whether your data is \"balanced\", i.e., whether it\n",
        "# has a similar amount of data for each class:\n",
        "from collections import Counter\n",
        "print(\"Total:\", Counter(data_all[\"Labels\"]))\n",
        "\n",
        "# And split the data into three parts:\n",
        "split_target_size = int(num_data_all/3)\n",
        "train = {\n",
        "    \"Text\": data_all[\"Text\"][:(split_target_size)],\n",
        "    \"Labels\": data_all[\"Labels\"][:(split_target_size)]\n",
        "}\n",
        "print(\"Train:\", Counter(train[\"Labels\"]))\n",
        "val = {\n",
        "    \"Text\": data_all[\"Text\"][(split_target_size):(2*split_target_size)],\n",
        "    \"Labels\": data_all[\"Labels\"][(split_target_size):(2*split_target_size)]\n",
        "}\n",
        "print(\"Val:  \", Counter(val[\"Labels\"]))\n",
        "test = {\n",
        "    \"Text\": data_all[\"Text\"][(2*split_target_size):],\n",
        "    \"Labels\": data_all[\"Labels\"][(2*split_target_size):]\n",
        "}\n",
        "print(\"Test: \", Counter(test[\"Labels\"]))\n",
        "print()\n",
        "\n",
        "assert len(train['Text']) == len(train['Labels']), \"Number of texts does not match number of labels for train data!\"\n",
        "assert len(val['Text']) == len(val['Labels']), \"Number of texts does not match number of labels for val data!\"\n",
        "assert len(test['Text']) == len(test['Labels']), \"Number of texts does not match number of labels for test data!\"\n",
        "\n",
        "print(f\"The train data has {len(train['Text'])} texts and {len(train['Labels'])} labels,\")\n",
        "print(f\"the validation data {len(val['Text'])} texts and {len(val['Labels'])} labels\")\n",
        "print(f\"and the test data {len(test['Text'])} texts and {len(test['Labels'])} labels.\")\n",
        "print()\n",
        "\n",
        "print(\"First five training texts:\")\n",
        "for i in range(5):\n",
        "    print(f\"'{train['Text'][i]}', {train['Labels'][i]}\")"
      ],
      "metadata": {
        "id": "xEqGgh3EvOb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee2e86a-b01e-4054-e7cc-b522739d2311"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: Counter({'neutral': 1424, 'positive': 1083, 'negative': 374})\n",
            "Train: Counter({'neutral': 457, 'positive': 384, 'negative': 119})\n",
            "Val:   Counter({'positive': 424, 'neutral': 379, 'negative': 157})\n",
            "Test:  Counter({'neutral': 588, 'positive': 275, 'negative': 98})\n",
            "\n",
            "The train data has 960 texts and 960 labels,\n",
            "the validation data 960 texts and 960 labels\n",
            "and the test data 961 texts and 961 labels.\n",
            "\n",
            "First five training texts:\n",
            "'WhEn I‚Äôm AbLe to PuRchAse A ChaNel BaG &amp; StiLl LivE comfortabLYüåªüí™üèæ', positive\n",
            "'(INFO) Taehyung is wearing CHANEL TOTE BAG ,GRAFFITI CANVASSILVER HARDWARE, from Spring/Summer 2014 (Limited Edition)\n",
            "\n",
            "-üêØüêÜ https://t.co/iqiWeD4BnV https://t.co/dGrOTWjnwL', positive\n",
            "'Influencers on insta are like \"I finally got offered my dream luxury bag\" and it's a black Chanel flap. üòÜ', negative\n",
            "'@Brieyonce This is like Chanel‚Äôs advent calendar including stickers, a dust bag and a keychain.', neutral\n",
            "'Tae with jw anderson jacket and chanel tote bagüòâ after jen using all these brands while touring in europe', positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, to avoid that our model just learns to pick the most common label (as that is the most likely to be the case if is is unsure), we balance the training set by oversampling so that it contains the same number of examples for each label."
      ],
      "metadata": {
        "id": "zjzJ1jAgja1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "counts = Counter(train['Labels'])\n",
        "max_count = max(counts.values())\n",
        "for label, count in counts.items():\n",
        "    label_indices = [i for i, train_label in enumerate(train['Labels']) if train_label == label]\n",
        "    selected_indices = random.choices(label_indices, k=max_count - count)\n",
        "    train['Text'] += [train['Text'][i] for i in selected_indices]\n",
        "    train['Labels'] += [train['Labels'][i] for i in selected_indices]\n",
        "\n",
        "# now shuffle the data again\n",
        "indices = random.sample(range(len(train['Text'])), len(train['Text']))\n",
        "train['Text'] = [train['Text'][i] for i in indices]\n",
        "train['Labels'] = [train['Labels'][i] for i in indices]\n",
        "print(\"Train:\", Counter(train[\"Labels\"]))"
      ],
      "metadata": {
        "id": "GWLdqgQHkA-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70341b42-577a-4f0b-fe12-2d9d7882571b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: Counter({'negative': 457, 'neutral': 457, 'positive': 457})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WQ19YMNEAuK"
      },
      "source": [
        "Great, the data is ready!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BYdtu44Xrhp"
      },
      "source": [
        "## Selecting a Pre-Trained Language Model\n",
        "\n",
        "In which language is the text in the data written? Many language models are trained to understand a particular language. In case you need to process text of just one language, taking such a specialized model is often a good idea. However, also cross-language models exist, which were trained on many different language. We use a cross-language model here, but you can comment out one of the other lines to use a language-specific model instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LHzEmeSnXq2w"
      },
      "outputs": [],
      "source": [
        "model_name = 'xlm-roberta-base'                        # for 100 languages\n",
        "# model_name = \"roberta-base\"                          # for English\n",
        "# model_name = \"benjamin/roberta-base-wechsel-german\"  # for German\n",
        "# model_name = \"camembert-base\"                        # for French"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq68uB4cbnKc"
      },
      "source": [
        "What is this ``model_name``? It relates to our second question:\n",
        "\n",
        "What is a **pre-trained language model**?\n",
        "\n",
        "A pre-trained language model has be trained to predict words in a text. Usually, single words are removed from a text to make kind of a cloze test. The model is trained to fill in the gap with the word that we removed. This process is done for millions of texts, making the model somewhat adapt at \"speaking\" the language(s). This is called pre-training, as it teaches the model a specific skill (e.g., language understanding) that is useful for many other skills (e.g., predicting the sentiment of a text).\n",
        "\n",
        "What even is **fine-tuning**?\n",
        "\n",
        "It means to take a pre-trained model and training it now on the actual task we want it to solve. Since the model already has useful skills (e.g., language understanding), we need less training data to transform it into an expert for the task. To highlight that this is a (relatively) small adjustment, this step is called \"fine-tuning\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pm-wgbhEarc"
      },
      "source": [
        "## Defining the Classification Task\n",
        "\n",
        "As said above, classification tasks can be separated into single-label and multi-label classification. This tutorial by default uses single-label classification, but it also contains the code for multi-label classification, in case that is what you need when you use your own data. Here is a more detailed distinction:\n",
        "\n",
        "1. Single-Label Classification\n",
        "\n",
        "  - Select one label from a list of possible labels.\n",
        "  - Example: Does this text have a *positive* or a *negative* sentiment?\n",
        "\n",
        "2. Multi-Label Classification\n",
        "  - Decide for each label from a list of possible labels whether it applies.\n",
        "  - Example: Is this text *positive*, *in formal language* and/or *easy to read*?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mP2b9HkBY1Qc"
      },
      "outputs": [],
      "source": [
        "# Classify by 'single_label_classification' or 'multi_label_classification'?\n",
        "problem_type =  'single_label_classification'\n",
        "\n",
        "# Ensure the labels align with the selected classification type\n",
        "if problem_type == 'single_label_classification':\n",
        "    assert isinstance(train['Labels'][0], str), (\n",
        "        \"For 'single-label' tasks, labels should be strings (e.g., 'positive').\"\n",
        "    )\n",
        "elif problem_type == 'multi_label_classification':\n",
        "    assert isinstance(train['Labels'][0], list), (\n",
        "        \"For 'multi-label' tasks, labels should be a list \" \\\n",
        "        + \"(e.g., ['positive', 'easy-to-read']).\"\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Invalid classification type: {problem_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always advised to check your data behaves like you would expect it to:"
      ],
      "metadata": {
        "id": "Ov4cgrSKIqS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x6AKMt2JiLd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e54548-eb07-4fc0-d1a3-e2e1a6b5b5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels (same across train, val and test): ['negative', 'neutral', 'positive']\n"
          ]
        }
      ],
      "source": [
        "def get_unique_labels(data):\n",
        "    \"\"\"\n",
        "    Returns the number of possible labels and the possible labels.\n",
        "    \"\"\"\n",
        "    def flatten_list(list_to_flatten):\n",
        "        return [x for xs in list_to_flatten for x in xs]\n",
        "\n",
        "    labels = data['Labels']\n",
        "    if problem_type == 'multi_label_classification':\n",
        "        labels = flatten_list(labels)\n",
        "\n",
        "    labels = set(labels)\n",
        "    return labels\n",
        "\n",
        "\n",
        "labels_train = get_unique_labels(train)\n",
        "labels_val = get_unique_labels(val)\n",
        "labels_test = get_unique_labels(test)\n",
        "\n",
        "assert labels_train == labels_val == labels_test, (\n",
        "    \"Train, val and test must contain the same labels, but the where\\n\"\n",
        "    f\"{labels_train},\\n{labels_val} and\\n{labels_test}, respectively\"\n",
        ")\n",
        "\n",
        "print(f\"Labels (same across train, val and test): {list(labels_train)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYAu-jSLtSBf"
      },
      "source": [
        "## Setting up the Model\n",
        "\n",
        "The next step is to load the model and tokenizer. The tokenizer translates text into a model-specific vocabulary (usually numbers) that the model can process efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zGtsc2tGtQWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d21854-469f-45bb-e5b0-e87434b465e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# The AI library uses integer numbers instead of strings for labels\n",
        "# We convert them into each other with these dictionaries\n",
        "id2label = {i: k for i, k in enumerate(labels_train)}  # id2label[0] == labels_train[0]\n",
        "label2id = {k: i for i, k in enumerate(labels_train)}  # label2id[labels_train[0]] == 0\n",
        "\n",
        "model_config = {'pretrained_model_name_or_path': model_name,\n",
        "                'num_labels': len(labels_train),\n",
        "                'problem_type': problem_type,\n",
        "                'id2label': id2label,\n",
        "                'label2id': label2id}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(**model_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLFf-h69n8Nn"
      },
      "source": [
        "Now we specify training-specific parameters. Do not worry if you are unsure about what to change if you use your own data. The preset values should work just fine for most cases.\n",
        "\n",
        "In general, when training a model, you provide it with some example data points (in your case, the training data). From this data, the model learns helpful patterns that explain the correlation between input and output.\n",
        "\n",
        "Key parameters of the training are:\n",
        "\n",
        "- **batch_size**: Determines how many examples we show to the model at a time before deducing how to improve classification.  \n",
        "- **learning_rate**: Controls how strongly the model commits to patterns it recognizes within each batch.  \n",
        "- **num_epoch**: Specifies how many times the model sees all the training data (e.g., 3 times).  \n",
        "- **warm_up_rate**: Indicates the portion of training during which the model makes smaller adjustments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BWLgh6LRGNOE"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 3  # how often we go through the entire training dataset\n",
        "warm_up_rate = 0.1  # fraction of our training steps for warm-up scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w37QzRN_U41u"
      },
      "source": [
        "Next we create dataloaders to prepare the data for the model training. This involves to convert both the texts and the labels from string to integers: tokenization (texts to sequences of numbers) and label2id mapping (labels to numbers). Both are then used in the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QIP_Jj34oz9r"
      },
      "outputs": [],
      "source": [
        "# Convert text into the model vocabulary (tokenization)\n",
        "train_text = tokenizer([m for m in train['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "val_text = tokenizer([m for m in val['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "test_text = tokenizer([m for m in test['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "\n",
        "# Convert label strings into label ids (using the label2id mapping)\n",
        "def map_labels(labels, mapping):\n",
        "    if isinstance(labels[0], list):  # multi-label classification\n",
        "        label_matrix = []\n",
        "        for text_labels in labels:  # text_labels: labels for one text\n",
        "            label_vector = torch.tensor([\n",
        "                # vector of 0s and 1s, the index encodes the label\n",
        "                1 if label in text_labels else 0 for label in mapping\n",
        "            ])\n",
        "            label_matrix.append(label_vector)\n",
        "        labels = torch.stack(label_matrix, dim=0)\n",
        "    return torch.tensor([mapping[label] for label in labels])\n",
        "\n",
        "train_y = map_labels(train['Labels'], label2id)\n",
        "val_y = map_labels(val['Labels'], label2id)\n",
        "test_y = map_labels(test['Labels'], label2id)\n",
        "\n",
        "# Get dataloaders for iteration over the data\n",
        "def generate_dataloader(text, y, batch_size, workers=1):\n",
        "    \"\"\"\n",
        "    Returns a dataloader with input_ids and attention_mask to process the text.\n",
        "    \"\"\"\n",
        "    attention_mask = text['attention_mask']\n",
        "    input_ids = text['input_ids']\n",
        "    dataset = list(zip(input_ids, attention_mask, y))\n",
        "    dataloader = DataLoader(\n",
        "        dataset, shuffle=True, batch_size=batch_size, num_workers=workers)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = generate_dataloader(train_text, train_y, batch_size)\n",
        "val_dataloader = generate_dataloader(val_text, val_y, batch_size)\n",
        "test_dataloader = generate_dataloader(test_text, test_y, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELylkOMArO-h"
      },
      "source": [
        "The learning of patterns and adaptation of the model is achieved by the optimizer. In our case it is a special optimizer that keeps a model from optimizing too much to the training data-which often results in less generalization to new data. You can find the technical details in [the SAM optimizer repository](https://github.com/davda54/sam)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fFuqFXajGfJ6"
      },
      "outputs": [],
      "source": [
        "optimizer = SAM(model.parameters(), torch.optim.Adam, lr=learning_rate, adaptive=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scheduler adapts the learning rate of the optimizer (by how much it adopts the model in a learning step) during the learning process. There are several different scheduling strategies, though typically they decrease the learning rate after some initial (\"warm up\") steps. The further one is in training, one typically assumes that only minor adoptions to the model are necessary to really hit the optimum."
      ],
      "metadata": {
        "id": "W1BRgRKM9zVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = (len(train['Labels']) // batch_size) * num_epochs\n",
        "num_warmup_steps = int(num_training_steps * warm_up_rate)\n",
        "\n",
        "scheduler = transformers.get_cosine_schedule_with_warmup(\n",
        "    optimizer = optimizer,\n",
        "    num_warmup_steps = num_warmup_steps,\n",
        "    num_training_steps = num_training_steps,\n",
        "    last_epoch = -1\n",
        ")"
      ],
      "metadata": {
        "id": "uCI9gyCP9z-I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start the training, let us check our available ressources and compare with the batch size (how much of the training data we load at once - if we have a small GPU only, larger batches might not fit into it at once)."
      ],
      "metadata": {
        "id": "In8FUQGL_RCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bUwFFftDV457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7069e7-b832-4a5b-f385-0a6b1b6c03f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Memory: 12.67 GB total, 3.21 GB available\n",
            "No GPU available - using CPU\n",
            "Model has 278,045,955 parameters\n",
            "Training on CPU takes too long. For the sake of the tutorial, we restrict the data to 10 examples each. The classifier will not really work from that little data, but illustrates how it would work in general.\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Memory and system check before training\n",
        "import psutil\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Set tokenizers parallelism to avoid fork warnings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Check system memory\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"System Memory: {memory.total / 1024**3:.2f} GB total, {memory.available / 1024**3:.2f} GB available\")\n",
        "\n",
        "\n",
        "# Check GPU memory if available\n",
        "if device != 'cpu':\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    gpu_memory_allocated = torch.cuda.memory_allocated(0)\n",
        "    gpu_memory_reserved = torch.cuda.memory_reserved(0)\n",
        "\n",
        "    print(f\"GPU Memory: {gpu_memory / 1024**3:.2f} GB total\")\n",
        "    print(f\"GPU Memory Allocated: {gpu_memory_allocated / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory Reserved: {gpu_memory_reserved / 1024**3:.2f} GB\")\n",
        "\n",
        "    # Clear any existing GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "else:\n",
        "    print(\"No GPU available - using CPU\")\n",
        "\n",
        "# Check model size\n",
        "model_size = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model has {model_size:,} parameters\")\n",
        "\n",
        "# Recommend batch size based on available memory\n",
        "if device != 'cpu':\n",
        "    available_gpu_memory = gpu_memory - gpu_memory_reserved\n",
        "    if available_gpu_memory < 4 * 1024**3:  # Less than 4GB available\n",
        "        recommended_batch_size = 8\n",
        "    elif available_gpu_memory < 8 * 1024**3:  # Less than 8GB available\n",
        "        recommended_batch_size = 16\n",
        "    else:\n",
        "        recommended_batch_size = 32\n",
        "\n",
        "    print(f\"Recommended batch size: {recommended_batch_size}\")\n",
        "\n",
        "    if batch_size > recommended_batch_size:\n",
        "        print(f\"Warning: Current batch size ({batch_size}) may be too large. Consider reducing to {recommended_batch_size}\")\n",
        "\n",
        "# Avoid that tutorial gets stuck when running on a CPU.\n",
        "if device == 'cpu':\n",
        "    print(\"Training on CPU takes too long. For the sake of the tutorial, we \" \\\n",
        "        \"restrict the data to 10 examples each. The classifier will not really \" \\\n",
        "        \"work from that little data, but illustrates how it would work in \" \\\n",
        "        \"general.\")\n",
        "    train_text = tokenizer([m for m in train['Text'][0:10]], truncation=True, padding='longest', return_tensors='pt')\n",
        "    val_text = tokenizer([m for m in val['Text'][0:10]], truncation=True, padding='longest', return_tensors='pt')\n",
        "    test_text = tokenizer([m for m in test['Text'][0:10]], truncation=True, padding='longest', return_tensors='pt')\n",
        "    train_y = map_labels(train['Labels'][0:10], label2id)\n",
        "    val_y = map_labels(val['Labels'][0:10], label2id)\n",
        "    test_y = map_labels(test['Labels'][0:10], label2id)\n",
        "    train_dataloader = generate_dataloader(train_text, train_y, 10)\n",
        "    val_dataloader = generate_dataloader(val_text, val_y, 10)\n",
        "    test_dataloader = generate_dataloader(test_text, test_y, 10)\n",
        "    num_epochs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_hiMENYrwM0"
      },
      "source": [
        "## Fine-Tuning the Pre-Trained Model\n",
        "\n",
        "(\"Fine-tuning\" means to train an already pre-trained model to a specific task)\n",
        "\n",
        "Let's start the training process!  \n",
        "\n",
        "During fine-tuning, we show the training data to the model and adjust its parameters to optimize performance for the task. Here is what happens:\n",
        "\n",
        "- The model learns patterns in the data to perform the classification task.  \n",
        "- After each **epoch** (a complete pass through the training dataset), we test the model on the validation set to monitor progress.  \n",
        "- The best-performing model is saved during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e35OhfF0JO52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a2fff6-79b6-471b-830f-84dd5bd464cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: Epoch 0, Train step 0, Loss 1.1550, learning_rate 8.33e-06\n",
            "Validation: Epoch 0, Train step 0, Loss 1.0439, old best/epoch nf/0\n",
            "**** END EPOCH 0 ****\n",
            "**** FINISHED TRAINING FOR N=1 ****\n",
            "BEST EPOCH: 0\n",
            "BEST LOSS: 1.0439118146896362\n"
          ]
        }
      ],
      "source": [
        "best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "already_trained = 0\n",
        "best_model_path = ''\n",
        "should_delete = True\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs): # Repeat num_epochs times\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader): # Train the model on the batch\n",
        "        try:\n",
        "            input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "\n",
        "            # First forward pass\n",
        "            output = model(input_ids, attention_mask, labels=y)\n",
        "            loss = output.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # SAM optimizer first step\n",
        "            optimizer.first_step(zero_grad=True)\n",
        "\n",
        "            # Second forward pass (required by SAM)\n",
        "            output2 = model(input_ids, attention_mask, labels=y)\n",
        "            loss2 = output2.loss\n",
        "            loss2.backward()\n",
        "\n",
        "            # SAM optimizer second step\n",
        "            optimizer.second_step(zero_grad=True)\n",
        "\n",
        "            # Update learning rate AFTER optimizer steps\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Train: Epoch {epoch}, Train step {already_trained+batch_idx}, Loss {loss.item():.4f}, learning_rate {scheduler.get_last_lr()[0]:.2e}\", flush=True)\n",
        "\n",
        "            # Clear cache periodically to prevent memory buildup\n",
        "            if batch_idx % 5 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"OOM Error at batch {batch_idx}. Trying to recover...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                # Try with smaller effective batch size\n",
        "                if batch_size > 8:\n",
        "                    batch_size = batch_size // 2\n",
        "                    print(f\"Reducing batch size to {batch_size}\")\n",
        "                    train_dataloader = generate_dataloader(train_text, train_y, batch_size)\n",
        "                    val_dataloader = generate_dataloader(val_text, val_y, batch_size)\n",
        "                    break\n",
        "                else:\n",
        "                    raise e\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    already_trained += batch_idx\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_dataloader): # Validate the current state of the model on the validation data\n",
        "            input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            val_output = model(input_ids, attention_mask, labels=y)\n",
        "            val_loss.append(val_output.loss)\n",
        "\n",
        "    val_loss = torch.mean(torch.stack(val_loss))\n",
        "\n",
        "    print(f\"Validation: Epoch {epoch}, Train step {already_trained}, Loss {val_loss.item():.4f}, old best/epoch {str(best_loss)[1:6]}/{best_epoch}\", flush=True)\n",
        "\n",
        "    if val_loss < best_loss: # Save the model if the val_loss is the best loss we have seen so far\n",
        "        best_loss = val_loss.item()\n",
        "        best_epoch = epoch\n",
        "        if should_delete and best_model_path and os.path.exists(best_model_path):\n",
        "            shutil.rmtree(best_model_path)\n",
        "        best_model_path = f\"./my_model_epoch_{best_epoch}_val_loss_{str(val_loss.item())[1:6]}\"\n",
        "        model.save_pretrained(best_model_path, from_pt=True)\n",
        "\n",
        "        print(f\"**** END EPOCH {epoch} ****\")\n",
        "\n",
        "    # Clean up memory after each epoch\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"**** FINISHED TRAINING FOR N={num_epochs} ****\")\n",
        "print(f\"BEST EPOCH: {best_epoch}\")\n",
        "print(f\"BEST LOSS: {best_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6IoeYsasTlt"
      },
      "source": [
        "The training is finished now we can load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UlEDLy1qCONM"
      },
      "outputs": [],
      "source": [
        "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8bn_AVsXY7"
      },
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "With the loaded model we can now predict the labels of test set's texts and compare thos predicted labels with the one already in the dataset to understand the models performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y_logits = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_dataloader):\n",
        "        input_ids, attention_mask = batch[0].to(device), batch[1].to(device)\n",
        "        batch_y_logits = best_model(input_ids, attention_mask).logits\n",
        "        test_y_logits.append(batch_y_logits)\n",
        "\n",
        "test_y_logits = torch.cat(test_y_logits, dim=0)\n",
        "print(test_y_logits)"
      ],
      "metadata": {
        "id": "hO4pA3i2CUTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c136d994-d1f5-419f-fac8-12c8b537c9ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0879, 0.2995, 0.2521],\n",
            "        [0.0882, 0.2895, 0.2490],\n",
            "        [0.0897, 0.2892, 0.2551],\n",
            "        [0.0912, 0.2996, 0.2517],\n",
            "        [0.0823, 0.2794, 0.2468],\n",
            "        [0.0802, 0.2764, 0.2462],\n",
            "        [0.0843, 0.2851, 0.2509],\n",
            "        [0.0834, 0.3031, 0.2491],\n",
            "        [0.0788, 0.2806, 0.2440],\n",
            "        [0.0832, 0.2869, 0.2490]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFBY_wgsg6wg"
      },
      "source": [
        "Now these \"logits\" have to be converted to (number) labels.\n",
        "\n",
        "Based on the selected ``problem_type``, the following code selects the correct decision function. The decision function converts the scores the model predicts into the actual decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nZD7XiARg6KS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bcb14c-3093-40ea-8f69-cec2e606f262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# In multi-label classification, every label for which the model predicts a\n",
        "# score above this value is said as being predicted\n",
        "multi_label_decision_threshold = 0\n",
        "\n",
        "decision_function = {\n",
        "    # select single label with highest score\n",
        "    'single_label_classification': lambda x: torch.argmax(x, dim=1),\n",
        "    # select all labels with score above the threshold\n",
        "    'multi_label_classification': lambda x: torch.where(\n",
        "        x > multi_label_decision_threshold, 1, 0)\n",
        "}[problem_type]\n",
        "\n",
        "test_y_pred = decision_function(test_y_logits).cpu()\n",
        "print(test_y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can finally compare how the predicted labels (`test_y_pred`) align with the labels that came along with the dataset (`test_y`). The scikit-learn library provides a nice report function to compare them."
      ],
      "metadata": {
        "id": "SP2I2CTWCuJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZXp0j0r5Tg4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f737d2-90d5-4d17-898d-8692f2512dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.00      0.00         1\n",
            "     neutral       0.80      1.00      0.89         8\n",
            "    positive       1.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80        10\n",
            "   macro avg       0.93      0.33      0.30        10\n",
            "weighted avg       0.84      0.80      0.71        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_y, test_y_pred, target_names=label2id.keys(), zero_division=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBm7kAZoAOz"
      },
      "source": [
        "### Results\n",
        "\n",
        "The classification report shows us four metric results these are the precision,\n",
        "the recall, the f1-score, and the accuracy. Additionally, the report displays two different average aggregations, these are the macro avg, and the weighted average.\n",
        "\n",
        "The *precision* tells us \"When we predict a label, is it the correct label?\"\n",
        "\n",
        "The *recall* tells us \"How many instances of a class do we find?\"\n",
        "\n",
        "The *f1-score* is the harmonic mean of the *precision* and the *recall*.\n",
        "\n",
        "The *accuracy* tells us \"How many of our predictions are correct?\"\n",
        "\n",
        "The *macro avg* aggregates the *f1-score* per class it tells us \"How well do we classify, if all classes occur equally often?\"\n",
        "\n",
        "The *weighted avg* aggregates the *f1-score* weighted by class size it tells us \"How well do we classify the complete label set?\"\n",
        "\n",
        "## Using the Classifier\n",
        "\n",
        "You can now check the classifier on further examples:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(model, texts):\n",
        "    tokenized_texts = tokenizer(texts, truncation=True, padding='longest', return_tensors='pt')\n",
        "    input_ids, attention_mask = tokenized_texts['input_ids'].to(device), tokenized_texts['attention_mask'].to(device)\n",
        "    logits = model(input_ids, attention_mask).logits\n",
        "    predictions = decision_function(logits).cpu().tolist()\n",
        "    predicted_labels = [id2label[prediction] for prediction in predictions]\n",
        "    return predicted_labels\n",
        "\n",
        "example_texts = [\"What a great bag!\", \"What a horrible thing\"]\n",
        "example_predicted_labels = classify(best_model, example_texts)\n",
        "for i in range(len(example_texts)):\n",
        "    print(f\"The classifier predicted the label '{example_predicted_labels[i]}' for the text '{example_texts[i]}'\")"
      ],
      "metadata": {
        "id": "WV6b2FuhNKqo",
        "outputId": "e3ebf01e-1ab0-4335-af93-0fc1acf2da64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The classifier predicted the label 'neutral' for the text 'What a great bag!'\n",
            "The classifier predicted the label 'neutral' for the text 'What a horrible thing'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Reading\n",
        "\n",
        "- [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/pdf/1911.02116)\n",
        "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692)\n",
        "- [CamemBERT: a Tasty French Language Model](https://arxiv.org/pdf/1911.03894)\n",
        "- [WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models](https://aclanthology.org/2022.naacl-main.293.pdf)\n",
        "- [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/pdf/2010.01412)"
      ],
      "metadata": {
        "id": "IR1ldehFV7xV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-G0nQF57ePk"
      },
      "source": [
        "## Contact Details\n",
        " For questions or feedback, contact Stephan Linzbach via [Stephan.Linzbach@gesis.org](mailto:Stephan.Linzbach@gesis.org)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "textclassification_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}