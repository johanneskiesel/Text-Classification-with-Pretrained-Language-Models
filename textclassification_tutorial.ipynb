{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8222d6ZMz8Nm"
      },
      "source": [
        "# A practical guide to multilingual large language  model (RoBERTa) classification\n",
        "\n",
        "This step-by-step tutorial provides a gentle introduction to customizing (fine-tuning) a pre-trained multilingual language model (RoBERTa) for text classification tasks. It demonstrates how to use the model's existing knowledge to classify text accurately, even with a small set of labeled examples. It takes input as JSON files with text documents and their corresponding labels for training, validating and testing. It covers using specialized models for English, German, and French while employing XLM-RoBERTa for over 100 additional languages.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "This tutorial answers the following questions:\n",
        "\n",
        "- What exactly is text-classification?\n",
        "- What is a pre-trained language model?\n",
        "- What even is fine-tuning?\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "-\tWork with large language models for text classification (RoBERTa)\n",
        "-\tCustomize (fine-tunine) a large language model for a text classification task in any language (100+ languages supported)\n",
        "-\tEmploy low-resource learning (with only few hundred examples) using the SAM optimizer\n",
        "\n",
        "## Target Audience\n",
        "\n",
        "This tutorial is aimed at an intermediate level. You should have basic knowledge of large language models and of Python programming.\n",
        "\n",
        "## Duration\n",
        "\n",
        "About half a work-day.\n",
        "\n",
        "## Use Cases\n",
        "\n",
        "- TODO\n",
        "-\tSocial scientists willing to learn about using large language models with basic prior understanding of it\n",
        "-\tSocial scientists with expertise in large-language models, interested in fine-tuning for multiple languages from only few examples.\n",
        "-\tComputer scientists interested in learning about how large-language models are used for social text classification.\n",
        "-\tAdvanced NLP researchers and professors looking for tutorials that can help their students in learning new topics.\n",
        "\n",
        "## Environment Setup\n",
        "\n",
        "Use this tutorial preferably in an environment with GPU access.\n",
        "\n",
        "Run the cells below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6KuYVJkLn_b0"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers pandas\n",
        "!wget --quiet https://raw.githubusercontent.com/davda54/sam/main/sam.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-ZpvTl_jHGEi"
      },
      "outputs": [],
      "source": [
        "# Import packages used in the tutorial\n",
        "\n",
        "import pandas\n",
        "from sam import SAM\n",
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "# Utility functions used in the tutorial\n",
        "\n",
        "def convert_in_output_size(labels, mapping):\n",
        "    label_resized = []\n",
        "    for l in labels:\n",
        "        tmp_l = torch.tensor([1 if k in l else 0 for k in mapping])\n",
        "        label_resized.append(tmp_l)\n",
        "    label_resized = torch.stack(label_resized, dim=0)\n",
        "    return label_resized\n",
        "\n",
        "def convert_labels(labels, mapping):\n",
        "    if isinstance(labels[0], list):\n",
        "        labels = convert_in_output_size(labels, mapping)\n",
        "    return torch.tensor([mapping[l] for l in labels])\n",
        "\n",
        "def flatten_list(list_to_flatten):\n",
        "  \"\"\"\n",
        "  Returns one list from a list of lists.\n",
        "  \"\"\"\n",
        "  return [x for xs in list_to_flatten for x in xs]\n",
        "\n",
        "def infer_output_size(data):\n",
        "  \"\"\"\n",
        "  Returns the number of possible labels and the possible labels.\n",
        "  \"\"\"\n",
        "  labels = data['Labels']\n",
        "  if isinstance(labels[0], list):\n",
        "    labels = flatten_list(labels)\n",
        "  labels = set(labels)\n",
        "  return len(labels), labels\n",
        "\n",
        "def generate_dataloader(text, y, batch_size, workers=1):\n",
        "  \"\"\"\n",
        "  Returns a dataloader with input_ids and attention_mask to process the text.\n",
        "  \"\"\"\n",
        "  attention_mask = text['attention_mask']\n",
        "  input_ids = text['input_ids']\n",
        "\n",
        "  dataset = list(zip(input_ids, attention_mask, y))\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      dataset, shuffle=True, batch_size=batch_size, num_workers=workers)\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUCEaj7BDygM"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "What exactly is **text-classification**?\n",
        "\n",
        "In text-classification we try to assign a property to a text.\n",
        "\n",
        "For example we are interested in classifying texts that are about fruits. We could easily find a dictionary with all fruits (e.g.: 'Apple', 'Banana', 'Pear' etc.). Everytime we recognize a word from this dictionary in a text we know this text is about fruits, right? However, this might not be true all the time. For example \"Apple designed the new pencil pro.\" is not about the fruit 'Apple' although we would recognize it as such with our dictionary approach. So the context of the word might be helpful (more on this later). Furthermore, dictionaries work only for the language in the dictionary. However, we aim for multilingual approaches in this tutorial and will thus replace dictionaries with AI methods later in this tutorial.\n",
        "\n",
        "Classification is obviously transferable to more than just fruits. People try to classify the sentiment of a text, the stance towards an entity expressed in a text, the topic of a text, the expressed emotion in a text, and many many more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCBUwjg5DwTW"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "We here focus on text-classification in its purest form, i.e., have as data points single texts, each of which is assigned a single label (or class) that our classifier has to predict (as opposed to multi-label classification, in which a text can be assigned none up to all available labels, see the \"Defining the Classification Task\" section). We use the following format for this tutorial:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'Text': [\"text1\", \"text2\", \"...\", \"textN\"]\n",
        "  'Labels': [\"label-of-text1\", \"label-of-text2\", \"...\", \"label-of-textN\"]\n",
        "}\n",
        "```\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'Text': ['Yesterday i ate an apple.', 'Yesterday I crashed my Apple.'],\n",
        "  'Labels': ['about_fruit', 'not_about_fruit']\n",
        "}\n",
        "```\n",
        "\n",
        "As in most classification settings, we separate our data into three parts:\n",
        "\n",
        "- train. Contains the data on which classifiers are based (\"training data\")\n",
        "- val. Contains the data based on which we select a classifier among the available ones (\"validation data\")\n",
        "- test. Contains the data to give us an estimate of the performance of the selected classifier (\"test data\")\n",
        "\n",
        "These different datasets should all be independent from each other, so that (1) we select the classifier that generalizes best to the validation data it does not know from the training data and (2) we get a solid performance estimate of the classifier in the wild from the test data (that was neither used to train nor to validate/select the classifier).\n",
        "\n",
        "We take an old [Twitter Sentiment Analysis](https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis) dataset for our dataset. But feel free to use your own data instead."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget --quiet http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\n",
        "#!unzip Sentiment-Analysis-Dataset.zip\n",
        "# We take only 6000 texts for this tutorial (a few texts are multi-line)\n",
        "!head -n 6003 \"Sentiment Analysis Dataset.csv\" > dataset.csv\n",
        "data = pandas.read_csv('dataset.csv')\n",
        "data.head()  # have a look at the first 5 texts"
      ],
      "metadata": {
        "id": "514Du_SGpdj1",
        "outputId": "53cf650c-0b40-41e7-88bd-6ff97a6e931b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ItemID  Sentiment SentimentSource  \\\n",
              "0       1          0    Sentiment140   \n",
              "1       2          0    Sentiment140   \n",
              "2       3          1    Sentiment140   \n",
              "3       4          0    Sentiment140   \n",
              "4       5          0    Sentiment140   \n",
              "\n",
              "                                       SentimentText  \n",
              "0                       is so sad for my APL frie...  \n",
              "1                     I missed the New Moon trail...  \n",
              "2                            omg its already 7:30 :O  \n",
              "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
              "4           i think mi bf is cheating on me!!!   ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f98263c4-bffa-446c-bec6-d3ca38e349af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentSource</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f98263c4-bffa-446c-bec6-d3ca38e349af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f98263c4-bffa-446c-bec6-d3ca38e349af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f98263c4-bffa-446c-bec6-d3ca38e349af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1a0e0b3b-61e9-434d-b78a-dad482edae39\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a0e0b3b-61e9-434d-b78a-dad482edae39')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1a0e0b3b-61e9-434d-b78a-dad482edae39 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 6000,\n  \"fields\": [\n    {\n      \"column\": \"ItemID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1732,\n        \"min\": 1,\n        \"max\": 6002,\n        \"num_unique_values\": 6000,\n        \"samples\": [\n          1783,\n          3918,\n          222\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SentimentSource\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Kaggle\",\n          \"Sentiment140\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SentimentText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6000,\n        \"samples\": [\n          \"- dark angel is so cool. its unfair. i'm gonna go cut myself now \",\n          \" life iS Good\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Sentiment` is the label, with values of 0 (negative sentiment) and 1 (positive sentiment). The Text is in the column `SentimentText`.\n",
        "\n",
        "We now transform it into our simple structure detailed above:"
      ],
      "metadata": {
        "id": "1q2ymRmNuilv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data to make sure the order bears no meaning that a classifier\n",
        "# could learn instead of focussing on the text\n",
        "data = data.sample(frac=1)  # Trick: sampling all without replacement == shuffle\n",
        "\n",
        "data_all = {\n",
        "    \"Text\": data[\"SentimentText\"].tolist(),\n",
        "    \"Labels\": [\"positive\" if sentiment == 1 else \"negative\" for sentiment in data[\"Sentiment\"].tolist()]\n",
        "}\n",
        "num_data_all = len(data_all[\"Text\"])\n",
        "\n",
        "# It is always good to check whether your data is \"balanced\", i.e., whether it\n",
        "# has a similar amount of data for each class:\n",
        "from collections import Counter\n",
        "print(\"Total:\", Counter(data_all[\"Labels\"]))\n",
        "\n",
        "# And split the data into three parts:\n",
        "split_target_size = int(num_data_all/3)\n",
        "train = {\n",
        "    \"Text\": data_all[\"Text\"][:(split_target_size)],\n",
        "    \"Labels\": data_all[\"Labels\"][:(split_target_size)]\n",
        "}\n",
        "print(\"Train:\", Counter(train[\"Labels\"]))\n",
        "val = {\n",
        "    \"Text\": data_all[\"Text\"][(split_target_size):(2*split_target_size)],\n",
        "    \"Labels\": data_all[\"Labels\"][(split_target_size):(2*split_target_size)]\n",
        "}\n",
        "print(\"Val:  \", Counter(val[\"Labels\"]))\n",
        "test = {\n",
        "    \"Text\": data_all[\"Text\"][(2*split_target_size):],\n",
        "    \"Labels\": data_all[\"Labels\"][(2*split_target_size):]\n",
        "}\n",
        "print(\"Test: \", Counter(test[\"Labels\"]))\n",
        "print()\n",
        "\n",
        "assert len(train['Text']) == len(train['Labels']), \"Number of texts does not match number of labels for train data!\"\n",
        "assert len(val['Text']) == len(val['Labels']), \"Number of texts does not match number of labels for val data!\"\n",
        "assert len(test['Text']) == len(test['Labels']), \"Number of texts does not match number of labels for test data!\"\n",
        "\n",
        "print(f\"The train data has {len(train['Text'])} texts and {len(train['Labels'])} labels,\")\n",
        "print(f\"the validation data {len(val['Text'])} texts and {len(val['Labels'])} labels\")\n",
        "print(f\"and the test data {len(test['Text'])} texts and {len(test['Labels'])} labels.\")"
      ],
      "metadata": {
        "id": "xEqGgh3EvOb2",
        "outputId": "44b6e26f-c1ec-420c-9266-d78a80367091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: Counter({'negative': 4082, 'positive': 1918})\n",
            "Train: Counter({'negative': 1337, 'positive': 663})\n",
            "Val:   Counter({'negative': 1352, 'positive': 648})\n",
            "Test:  Counter({'negative': 1393, 'positive': 607})\n",
            "\n",
            "The train data has 2000 texts and 2000 labels,\n",
            "the validation data 2000 texts and 2000 labels\n",
            "and the test data 2000 texts and 2000 labels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WQ19YMNEAuK"
      },
      "source": [
        "Great, the data is ready!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BYdtu44Xrhp"
      },
      "source": [
        "## Selecting a Pre-Trained Language Model\n",
        "\n",
        "In which language is the text in the data written? Many language models are trained to understand a particular language. In case you need to process text of just one language, taking such a specialized model is often a good idea. However, also cross-language models exist, which were trained on many different language. We use a cross-language model here, but you can comment out one of the other lines to use a language-specific model instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "LHzEmeSnXq2w"
      },
      "outputs": [],
      "source": [
        "model_name = 'xlm-roberta-base'                        # for 100 languages\n",
        "# model_name = \"roberta-base\"                          # for English\n",
        "# model_name = \"benjamin/roberta-base-wechsel-german\"  # for German\n",
        "# model_name = \"camembert-base\"                        # for French"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq68uB4cbnKc"
      },
      "source": [
        "What is this ``model_name``? It relates to our second question:\n",
        "\n",
        "What is a **pre-trained language model**?\n",
        "\n",
        "A pre-trained language model has be trained to predict words in a text. Usually, single words are removed from a text to make kind of a cloze test. The model is trained to fill in the gap with the word that we removed. This process is done for millions of texts, making the model somewhat adapt at \"speaking\" the language(s). This is called pre-training, as it teaches the model a specific skill (e.g., language understanding) that is useful for many other skills (e.g., predicting the sentiment of a text).\n",
        "\n",
        "What even is **fine-tuning**?\n",
        "\n",
        "It means to take a pre-trained model and training it now on the actual task we want it to solve. Since the model already has useful skills (e.g., language understanding), we need less training data to transform it into an expert for the task. To highlight that this is a (relatively) small adjustment, this step is called \"fine-tuning\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhm0nAieFGRl"
      },
      "source": [
        "## Defining the Classification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pm-wgbhEarc"
      },
      "source": [
        "\n",
        "### Choosing vs. Deciding\n",
        "\n",
        "Before starting, it’s essential to clarify what type of classification task you want to perform. We distinguish between two main tasks:\n",
        "\n",
        "#### 1. Choosing (Single-Label Classification)\n",
        "- **Example:** *What is your favorite fruit?*\n",
        "- You select one correct label from a list of possible options.\n",
        "\n",
        "#### 2. Deciding (Multi-Label Classification)\n",
        "- **Example:** *Do you like apples?*\n",
        "- You evaluate each label independently and decide whether it applies.\n",
        "\n",
        "\n",
        "In our dataset, we have **single labels**, so we will insert `'choosing'` as the classification type. Here's the code for defining your task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP2b9HkBY1Qc"
      },
      "outputs": [],
      "source": [
        "# Prompt the user to select the classification type\n",
        "decision_type =  'choosing' # classify by 'choosing' or 'deciding'?\n",
        "\n",
        "# Validate the input\n",
        "assert decision_type in ['choosing', 'deciding'], \"Invalid input! Please enter 'choosing' or 'deciding'.\"\n",
        "\n",
        "# Ensure the labels align with the selected classification type\n",
        "if decision_type == 'deciding':\n",
        "    assert isinstance(train['Labels'][0], list), (\n",
        "        \"For 'deciding', labels should be a list (e.g., ['apple', 'banana']).\"\n",
        "    )\n",
        "else:\n",
        "    assert not isinstance(train['Labels'][0], list), (\n",
        "        \"For 'choosing', each label should be a single value (e.g., 'apple').\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFBY_wgsg6wg"
      },
      "source": [
        "### Choosing the Loss and Decision Functions\n",
        "\n",
        "Based on your ``decision_type`` we will now choose the correct loss function and the decision function.\n",
        "The loss function tells the model how well it achieved your task.\n",
        "The decision function tells us how to convert the model guesses in the actual decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZD7XiARg6KS"
      },
      "outputs": [],
      "source": [
        "losses = {'deciding': \"multi_label_classification\", #torch.nn.CrossEntropyLoss(),\n",
        "          'choosing': \"single_label_classification\",}\n",
        "\n",
        "decisions = {'deciding': lambda x: torch.where(x > 0, 1, 0),\n",
        "             'choosing': lambda x: torch.argmax(x, dim=1)}\n",
        "\n",
        "objective = losses[decision_type]\n",
        "decision_function = decisions[decision_type]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcFgWRZiM8o"
      },
      "source": [
        "### Fixing the Number of Possible Answers:\n",
        "\n",
        "The next question we need to clarify is: How many different labels are possible for your task?\n",
        "\n",
        "Example:\n",
        "\n",
        "Choose your favorite fruit from this list:\n",
        "  ```python\n",
        "  poss_labels = ['Banana', 'Apple', 'Pear', 'Peach'`]\n",
        "  model_output = [0.4, 0.5, -0.1, 0.7]\n",
        "  ```\n",
        "\n",
        "Or decide if you like the particular fruit:\n",
        "  ```python\n",
        "  poss_labels = ['Banana', 'Apple', 'Pear', 'Peach'`]\n",
        "  model_output = [0.4, 0.5, -0.1, 0.7]\n",
        "  ```\n",
        "\n",
        "In both cases we have **4** possible labels.\n",
        "In the first case we choose the fruit where the model signals the biggest aggreement **'Peach'** in the second we decide for each fruit if we like it by chosing to like everything above 0. Therefore, our example output tells us that we like **'Banana'**, **'Apple'**, and **'Peach'**.\n",
        "\n",
        "\n",
        "\n",
        "Let's find out how many labels are possible in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6AKMt2JiLd7",
        "outputId": "38d9f462-6259-45ad-b841-1f1d0a499d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your task distinguishes 2 different labels. These are: {'is_correct', 'is_incorrect'}\n"
          ]
        }
      ],
      "source": [
        "# Ask user to define the number of possible labels\n",
        "output_size = \"2\" # Number of labels\n",
        "\n",
        "# Ensure the user input is a valid integer\n",
        "def new_func(output_size):\n",
        "    assert output_size.isdigit(), \"Output_size needs to be a natural number.\"\n",
        "\n",
        "new_func(output_size)\n",
        "output_size = int(output_size)\n",
        "\n",
        "# Infer the number of labels and possible labels from the datasets\n",
        "infered_output_size_train, possible_labels_train = infer_output_size(train)\n",
        "infered_output_size_val, possible_labels_val = infer_output_size(val)\n",
        "infered_output_size_test, possible_labels_test = infer_output_size(test)\n",
        "\n",
        "# Determine the maximum number of inferred labels\n",
        "possible_labels = max(infered_output_size_train, infered_output_size_val, infered_output_size_test)\n",
        "\n",
        "# Warn the user if datasets have inconsistent labels\n",
        "if not (possible_labels_train == possible_labels_val == possible_labels_test):\n",
        "    print(\n",
        "        f\"Warning: Train contains {possible_labels_train} possible labels, \"\n",
        "        f\"Val contains {possible_labels_val} possible labels, and \"\n",
        "        f\"Test contains {possible_labels_test} possible labels. \"\n",
        "        \"This inconsistency is not recommended. All datasets should ideally contain the same labels.\"\n",
        "    )\n",
        "\n",
        "# Ensure the output size matches the inferred number of labels\n",
        "assert output_size == possible_labels, (\n",
        "    f\"We inferred {possible_labels} with the following possible labels: {possible_labels_train}.\"\n",
        ")\n",
        "assert possible_labels_train == possible_labels_val == possible_labels_test, (\n",
        "    \"Make sure that train, val, and test labels are equal!\"\n",
        ")\n",
        "\n",
        "# Create mapping dictionaries for labels and IDs\n",
        "id2label = {i: k for i, k in enumerate(possible_labels_train)}\n",
        "label2id = {k: i for i, k in enumerate(possible_labels_train)}\n",
        "\n",
        "print(f\"Your task distinguishes {output_size} different labels. These are: {possible_labels_train}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYAu-jSLtSBf"
      },
      "source": [
        "## 4) Setting up the Model\n",
        "\n",
        "\n",
        "Fantastic!\n",
        "\n",
        "We are close.\n",
        "We clarified the language, the objective and number of possible answers.\n",
        "\n",
        "Now, let’s load the necessary model and tokenizer.  \n",
        "The **tokenizer** translates the language into a model-specific vocabulary that the model can process efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGtsc2tGtQWi"
      },
      "outputs": [],
      "source": [
        "model_config = {'pretrained_model_name_or_path': model_name,\n",
        "                'num_labels': output_size,\n",
        "                'problem_type': objective,\n",
        "                'id2label': id2label,\n",
        "                'label2id': label2id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "1ebedda1ae3e4496b70ce5580ac09149",
            "73a38f3157a84e6787570a24f0b1556f",
            "0fc91d76b14d4a8d89b9dd3a21f5450c",
            "c4f6ec88c3014b6bbba1fd8515ff1507",
            "6912ccbab616408da9887a6d9967a4ae",
            "44ce5c02767c4e98b1d0af7fdb3928c2"
          ]
        },
        "id": "IDYrCWQwHBS4",
        "outputId": "4477a192-3a39-42f4-da32-49e5be5434c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ebedda1ae3e4496b70ce5580ac09149",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73a38f3157a84e6787570a24f0b1556f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fc91d76b14d4a8d89b9dd3a21f5450c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4f6ec88c3014b6bbba1fd8515ff1507",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6912ccbab616408da9887a6d9967a4ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44ce5c02767c4e98b1d0af7fdb3928c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(**model_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLFf-h69n8Nn"
      },
      "source": [
        "### Training Specific Settings\n",
        "\n",
        "Now we need to specify some training-specific parameters.  \n",
        "Don’t worry if you’re unsure about what to change—the preset values should work just fine for most cases.\n",
        "\n",
        "#### A Quick Intuition on Training\n",
        "\n",
        "When training a model, you provide it with some example data points (in your case, the training data). From this data, the model learns helpful patterns that explain the correlation between input and output.  \n",
        "To make the most of the data:  \n",
        "- We feed small portions (batches) to the model at a time, controlled by the **batch_size**.  \n",
        "- The model uses these batches to infer patterns but commits to those patterns cautiously, guided by the **learning rate (lr)**.  \n",
        "- To ensure the model retains knowledge from pre-training, we use a **warm-up rate**, which helps the model transition smoothly without forgetting its pre-trained knowledge.  \n",
        "\n",
        "#### Key Parameters\n",
        "Let’s go through each parameter one by one:  \n",
        "- **batch_size**: Determines how many examples we show to the model before deducing rules to improve classification.  \n",
        "- **learning_rate (lr)**: Controls how strongly the model commits to patterns it recognizes within each batch.  \n",
        "- **num_epoch**: Specifies how many times the model sees all the training data (e.g., 3 times).  \n",
        "- **warm_up_rate**: Indicates the portion of training during which the model makes smaller adjustments.  \n",
        "- **Device**: Refers to the hardware used for training. If you have a GPU, your training will be much faster.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWLgh6LRGNOE"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "num_epochs = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "warm_up_rate = 0.1\n",
        "num_training_steps = (len(train['Labels'])//batch_size)*num_epochs\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w37QzRN_U41u"
      },
      "source": [
        "\n",
        "\n",
        "For now we only need the train-set to teach the model and the val-set to decide if we taught our model well.\n",
        "Lets first translate our text into the models vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP_Jj34oz9r"
      },
      "outputs": [],
      "source": [
        "# Padding tokenizing text aka. translating text into the model vocabulary\n",
        "\n",
        "train_text = tokenizer([m for m in train['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "val_text = tokenizer([m for m in val['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "test_text = tokenizer([m for m in test['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "\n",
        "# Using the label2id mapping to convert the label strings into label ids\n",
        "train_y = convert_labels(train['Labels'], label2id)\n",
        "val_y = convert_labels(val['Labels'], label2id)\n",
        "test_y = convert_labels(test['Labels'], label2id)\n",
        "\n",
        "# Retrieve Dataloaders for fast iteration over the data\n",
        "train_dataloader = generate_dataloader(train_text, train_y, batch_size)\n",
        "val_dataloader = generate_dataloader(val_text, val_y, batch_size)\n",
        "test_dataloader = generate_dataloader(test_text, test_y, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELylkOMArO-h"
      },
      "source": [
        "The learning of patterns and adaptation of the model is achieved by the optimizer. In our case it is a special optimizer that keeps a model from optimizing. If you are really interested you can read more about it [here](https://github.com/davda54/sam). The scheduler adapts the learning rate according the warm_up_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFuqFXajGfJ6"
      },
      "outputs": [],
      "source": [
        "# Initialize optimizer and scheduler\n",
        "optimizer = SAM(model.parameters(), torch.optim.Adam, lr=lr, adaptive=True)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer = optimizer,\n",
        "                                          num_warmup_steps = num_training_steps*warm_up_rate,\n",
        "                                          num_training_steps = num_training_steps,\n",
        "                                          last_epoch = -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_hiMENYrwM0"
      },
      "source": [
        "## 5) Fine-tuning (Training)\n",
        "\n",
        "Let’s start the training process!  \n",
        "\n",
        "During fine-tuning, we show the training data to the model and adjust its parameters to optimize performance for the task.  \n",
        "Here’s what happens:  \n",
        "- The model learns patterns in the data to perform the classification task.  \n",
        "- After each **epoch** (a complete pass through the training dataset), we test the model on the validation set to monitor progress.  \n",
        "- The best-performing model is saved during the training process.\n",
        "\n",
        "Once training is complete, you will have a well-trained model ready for use.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUwFFftDV457",
        "outputId": "f1dc2b7e-7818-4690-cb2e-8a583d51fb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Memory: 184.25 GB total, 159.00 GB available\n",
            "No GPU available - using CPU\n",
            "Model has 124,647,170 parameters\n"
          ]
        }
      ],
      "source": [
        "# Memory and system check before training\n",
        "import psutil\n",
        "import gc\n",
        "# Trainings-loop with memory management fixes\n",
        "import os\n",
        "\n",
        "# Set tokenizers parallelism to avoid fork warnings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Check system memory\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"System Memory: {memory.total / 1024**3:.2f} GB total, {memory.available / 1024**3:.2f} GB available\")\n",
        "\n",
        "# Check GPU memory if available\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    gpu_memory_allocated = torch.cuda.memory_allocated(0)\n",
        "    gpu_memory_reserved = torch.cuda.memory_reserved(0)\n",
        "\n",
        "    print(f\"GPU Memory: {gpu_memory / 1024**3:.2f} GB total\")\n",
        "    print(f\"GPU Memory Allocated: {gpu_memory_allocated / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory Reserved: {gpu_memory_reserved / 1024**3:.2f} GB\")\n",
        "\n",
        "    # Clear any existing GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "else:\n",
        "    print(\"No GPU available - using CPU\")\n",
        "\n",
        "# Check model size\n",
        "model_size = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model has {model_size:,} parameters\")\n",
        "\n",
        "# Recommend batch size based on available memory\n",
        "if torch.cuda.is_available():\n",
        "    available_gpu_memory = gpu_memory - gpu_memory_reserved\n",
        "    if available_gpu_memory < 4 * 1024**3:  # Less than 4GB available\n",
        "        recommended_batch_size = 8\n",
        "    elif available_gpu_memory < 8 * 1024**3:  # Less than 8GB available\n",
        "        recommended_batch_size = 16\n",
        "    else:\n",
        "        recommended_batch_size = 32\n",
        "\n",
        "    print(f\"Recommended batch size: {recommended_batch_size}\")\n",
        "\n",
        "    if batch_size > recommended_batch_size:\n",
        "        print(f\"Warning: Current batch size ({batch_size}) may be too large. Consider reducing to {recommended_batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e35OhfF0JO52"
      },
      "outputs": [],
      "source": [
        "\n",
        "best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "already_trained = 0\n",
        "best_model_path = ''\n",
        "should_delete = True\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs): # Repeat num_epochs times\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader): # Train the model on the batch\n",
        "        try:\n",
        "            input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "\n",
        "            # First forward pass\n",
        "            output = model(input_ids, attention_mask, labels=y)\n",
        "            loss = output.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # SAM optimizer first step\n",
        "            optimizer.first_step(zero_grad=True)\n",
        "\n",
        "            # Second forward pass (required by SAM)\n",
        "            output2 = model(input_ids, attention_mask, labels=y)\n",
        "            loss2 = output2.loss\n",
        "            loss2.backward()\n",
        "\n",
        "            # SAM optimizer second step\n",
        "            optimizer.second_step(zero_grad=True)\n",
        "\n",
        "            # Update learning rate AFTER optimizer steps\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Train: Epoch {epoch}, Train step {already_trained+batch_idx}, Loss {loss.item():.4f}, learning_rate {scheduler.get_last_lr()[0]:.2e}\", flush=True)\n",
        "\n",
        "            # Clear cache periodically to prevent memory buildup\n",
        "            if batch_idx % 5 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"OOM Error at batch {batch_idx}. Trying to recover...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                # Try with smaller effective batch size\n",
        "                if batch_size > 8:\n",
        "                    batch_size = batch_size // 2\n",
        "                    print(f\"Reducing batch size to {batch_size}\")\n",
        "                    train_dataloader = generate_dataloader(train_text, train_y, batch_size)\n",
        "                    val_dataloader = generate_dataloader(val_text, val_y, batch_size)\n",
        "                    break\n",
        "                else:\n",
        "                    raise e\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    already_trained += batch_idx\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_dataloader): # Validate the current state of the model on the validation data\n",
        "            input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            val_output = model(input_ids, attention_mask, labels=y)\n",
        "            val_loss.append(val_output.loss)\n",
        "\n",
        "    val_loss = torch.mean(torch.stack(val_loss))\n",
        "\n",
        "    print(f\"Validation: Epoch {epoch}, Train step {already_trained}, Loss {val_loss.item():.4f}, old best/epoch {str(best_loss)[1:6]}/{best_epoch}\", flush=True)\n",
        "\n",
        "    if val_loss < best_loss: # Save the model if the val_loss is the best loss we have seen so far\n",
        "        best_loss = val_loss.item()\n",
        "        best_epoch = epoch\n",
        "        if should_delete and best_model_path and os.path.exists(best_model_path):\n",
        "            shutil.rmtree(best_model_path)\n",
        "        best_model_path = f\"./my_model_epoch_{best_epoch}_val_loss_{str(val_loss.item())[1:6]}\"\n",
        "        model.save_pretrained(best_model_path, from_pt=True)\n",
        "\n",
        "        print(f\"**** END EPOCH {epoch} ****\")\n",
        "\n",
        "    # Clean up memory after each epoch\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"**** FINISHED TRAINING FOR N={num_epochs} ****\")\n",
        "print(f\"BEST EPOCH: {best_epoch}\")\n",
        "print(f\"BEST LOSS: {best_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6IoeYsasTlt"
      },
      "source": [
        "The training is finished now we can load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlEDLy1qCONM"
      },
      "outputs": [],
      "source": [
        "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8bn_AVsXY7"
      },
      "source": [
        "## 6) Evaluation\n",
        "\n",
        "Finally, with the loaded model we can now predict our results for the unseen test set to understand the models performance in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF0nHp1aRaHc"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_idx, batch in enumerate(test_dataloader):\n",
        "    input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "    y_pred.append(model(input_ids, attention_mask, labels=y).logits)\n",
        "\n",
        "\n",
        "y_pred = torch.cat(y_pred, dim=0)\n",
        "y_pred = decision_function(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXp0j0r5Tg4y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_y, y_pred, target_names=label2id.keys(), zero_division=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBm7kAZoAOz"
      },
      "source": [
        "### Results\n",
        "\n",
        "The classification report shows us four metric results these are the precision,\n",
        "the recall, the f1-score, and the accuracy. Additionally, the report displays two different average aggregations, these are the macro avg, and the weighted average.\n",
        "\n",
        "The *precision* tells us \"When we predict a label, is it the correct label?\".\n",
        "\n",
        "The *recall* tells us \"How many instances of a class do we find?\".\n",
        "\n",
        "The *f1-score* is the harmonic mean of the *precision* and the *recall*.\n",
        "\n",
        "The *accuracy* tells us \"How many of our predictions are correct?\".\n",
        "\n",
        "The *macro avg* aggregates the *f1-score* per class it tells us \"How well do we classify, if all classes occur equally often.\".\n",
        "\n",
        "The *weighted avg* aggregates the *f1-score* weighted by class size it tells us \"How well do we classify the complete label set.\".\n",
        "\n",
        "\n",
        "### Analysis\n",
        "We can see that we have two classes 'is_correct' and 'is_incorrect'.\n",
        "We have one instance with from the class 'is_correct' and two from the class 'is_incorrect'.\n",
        "Our model does not learn to predict the class 'is_correct'.\n",
        "We find all instances of the class 'is_incorrect'.\n",
        "You can see this by the fact that the *recall* is 1.0.\n",
        "The *precision* is at 0.67, as one instance that we predict to be 'is_incorrect' is actually an instance of the class 'is_correct'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Reading\n",
        "\n",
        "- [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/pdf/1911.02116)\n",
        "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692)\n",
        "- [CamemBERT: a Tasty French Language Model](https://arxiv.org/pdf/1911.03894)\n",
        "- [WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models](https://aclanthology.org/2022.naacl-main.293.pdf)\n",
        "- [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/pdf/2010.01412)"
      ],
      "metadata": {
        "id": "IR1ldehFV7xV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-G0nQF57ePk"
      },
      "source": [
        "## Contact Details\n",
        " For questions or feedback, contact Stephan Linzbach via [Stephan.Linzbach@gesis.org](mailto:Stephan.Linzbach@gesis.org)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "textclassification_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}